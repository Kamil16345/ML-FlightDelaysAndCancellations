{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projekt Uczenia Maszynowego - Predykcja Op√≥≈∫nie≈Ñ Lot√≥w\n",
    "## Semestr letni 2024/25\n",
    "\n",
    "### Sk≈Çad grupy i podzia≈Ç zada≈Ñ\n",
    "\n",
    "- **Kamil Arkit, Dawid Chomiak**: Znalezienie i przygotowanie danych\n",
    "- **Kamil Arkit, Dominik Sob√≥tka**: Trenowanie, testowanie i ocena modelu 1\n",
    "- **Dawid Chomiak, ≈Åukasz Guziczak**: Trenowanie, testowanie i ocena modelu 2\n",
    "- **Wszyscy**: Sprawozdanie i wnioski\n",
    "\n",
    "## 1. Opis danych i motywacja autor√≥w\n",
    "\n",
    "### Zbi√≥r danych\n",
    "Wykorzystali≈õmy zbi√≥r danych \"Flight Delays and Cancellations\" z platformy Kaggle (US Department of Transportation), zawierajƒÖcy szczeg√≥≈Çowe informacje o lotach w USA za 2015 rok. Dataset sk≈Çada siƒô z ponad 5.8 miliona rekord√≥w lot√≥w z 31 atrybutami.\n",
    "\n",
    "**≈πr√≥d≈Ço**: https://www.kaggle.com/datasets/usdot/flight-delays\n",
    "\n",
    "### Motywacja\n",
    "Wyb√≥r tego problemu by≈Ç motywowany:\n",
    "- Praktycznym zastosowaniem w bran≈ºy lotniczej dla optymalizacji operacji\n",
    "- Znaczeniem ekonomicznym - op√≥≈∫nienia kosztujƒÖ bran≈ºe miliardy dolar√≥w rocznie\n",
    "- Korzy≈õciami dla pasa≈ºer√≥w - lepsze planowanie podr√≥≈ºy\n",
    "- Bogatym zbiorem danych umo≈ºliwiajƒÖcym zastosowanie r√≥≈ºnych technik ML\n",
    "- Aktualno≈õciƒÖ problemu - op√≥≈∫nienia lot√≥w dotykajƒÖ miliony podr√≥≈ºnych\n",
    "\n",
    "### G≈Ç√≥wne pliki danych:\n",
    "- `flights.csv` - dane o lotach (~5.8M rekord√≥w)\n",
    "- `airlines.csv` - informacje o liniach lotniczych\n",
    "- `airports.csv` - dane o lotniskach\n",
    "\n",
    "### Kluczowe atrybuty (flights.csv):\n",
    "- `YEAR`, `MONTH`, `DAY` - data lotu\n",
    "- `DAY_OF_WEEK` - dzie≈Ñ tygodnia\n",
    "- `AIRLINE` - kod linii lotniczej\n",
    "- `FLIGHT_NUMBER` - numer lotu\n",
    "- `ORIGIN_AIRPORT`, `DESTINATION_AIRPORT` - lotniska wylotu i przylotu\n",
    "- `SCHEDULED_DEPARTURE`, `DEPARTURE_TIME` - planowany i rzeczywisty czas wylotu\n",
    "- `DEPARTURE_DELAY` - op√≥≈∫nienie wylotu (w minutach)\n",
    "- `SCHEDULED_ARRIVAL`, `ARRIVAL_TIME` - planowany i rzeczywisty czas przylotu\n",
    "- `ARRIVAL_DELAY` - op√≥≈∫nienie przylotu (w minutach)\n",
    "- `CANCELLED` - czy lot zosta≈Ç odwo≈Çany\n",
    "- `DISTANCE` - dystans lotu\n",
    "- `AIR_TIME` - czas lotu w powietrzu\n",
    "\n",
    "### Zmienna docelowa:\n",
    "`DEPARTURE_DELAY` przekszta≈Çcone na klasyfikacjƒô binarnƒÖ: op√≥≈∫nienie >15 minut = 1, inaczej = 0\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Szybki start\n",
    "\n",
    "### Wymagania:\n",
    "```bash\n",
    "pip install pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm imbalanced-learn kagglehub joblib\n",
    "```\n",
    "\n",
    "### Automatyczne pobieranie danych:\n",
    "- Notebook automatycznie pobierze dane z Kaggle przy pierwszym uruchomieniu\n",
    "- Wymagane: konto Kaggle i token API ([instrukcja](https://github.com/Kaggle/kagglehub))\n",
    "- Dataset: [US Flight Delays](https://www.kaggle.com/datasets/usdot/flight-delays)\n",
    "\n",
    "### Alternatywnie - pobierz dane rƒôcznie:\n",
    "```python\n",
    "import kagglehub\n",
    "kagglehub.dataset_download(\"usdot/flight-delays\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatyczne pobieranie danych - uruchom tƒô kom√≥rkƒô najpierw!\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Sprawd≈∫ czy mamy kagglehub\n",
    "try:\n",
    "    import kagglehub\n",
    "except ImportError:\n",
    "    print(\"Instalujƒô kagglehub...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"kagglehub\"])\n",
    "    import kagglehub\n",
    "\n",
    "# Pr√≥buj znale≈∫ƒá dane lokalnie najpierw\n",
    "possible_paths = [\n",
    "    'data',  # lokalny folder\n",
    "    '../data',  # folder wy≈ºej\n",
    "    os.path.join(os.getcwd(), 'data'),\n",
    "]\n",
    "\n",
    "DATASET_PATH = None\n",
    "\n",
    "# Sprawd≈∫ lokalne foldery\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path) and os.path.exists(os.path.join(path, 'flights.csv')):\n",
    "        DATASET_PATH = path\n",
    "        print(f\"‚úì Znaleziono dane lokalnie w: {DATASET_PATH}\")\n",
    "        break\n",
    "\n",
    "# Je≈õli nie znaleziono lokalnie, pobierz z Kaggle\n",
    "if DATASET_PATH is None:\n",
    "    print(\"üì• Pobieram dane z Kaggle (to mo≈ºe chwilƒô potrwaƒá za pierwszym razem)...\")\n",
    "    try:\n",
    "        DATASET_PATH = kagglehub.dataset_download(\"usdot/flight-delays\")\n",
    "        print(f\"‚úì Dane pobrane do: {DATASET_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå B≈ÇƒÖd pobierania: {e}\")\n",
    "        print(\"\\nüîß RozwiƒÖzania:\")\n",
    "        print(\"1. Upewnij siƒô, ≈ºe masz konto Kaggle i skonfigurowany token API\")\n",
    "        print(\"   - Zaloguj siƒô na https://www.kaggle.com\")\n",
    "        print(\"   - Id≈∫ do Account -> Create New API Token\")\n",
    "        print(\"   - Zapisz plik kaggle.json w ~/.kaggle/ (Linux/Mac) lub C:\\\\Users\\\\[username]\\\\.kaggle\\\\ (Windows)\")\n",
    "        print(\"\\n2. Lub pobierz dane rƒôcznie:\")\n",
    "        print(\"   - https://www.kaggle.com/datasets/usdot/flight-delays\")\n",
    "        print(\"   - Rozpakuj do folderu 'data' obok tego notebooka\")\n",
    "        raise\n",
    "\n",
    "# Sprawd≈∫ czy pliki istniejƒÖ\n",
    "required_files = ['flights.csv', 'airlines.csv', 'airports.csv']\n",
    "missing_files = []\n",
    "for file in required_files:\n",
    "    if not os.path.exists(os.path.join(DATASET_PATH, file)):\n",
    "        missing_files.append(file)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"‚ùå Brakuje plik√≥w: {missing_files}\")\n",
    "    raise FileNotFoundError(f\"Nie znaleziono wymaganych plik√≥w: {missing_files}\")\n",
    "else:\n",
    "    print(\"‚úÖ Wszystkie pliki danych sƒÖ dostƒôpne!\")\n",
    "    print(f\"üìÅ Lokalizacja: {os.path.abspath(DATASET_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import bibliotek\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, f1_score, recall_score, \n",
    "    precision_score, confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Konfiguracja\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Biblioteki za≈Çadowane pomy≈õlnie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Wczytanie danych i analiza eksploracyjna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytanie danych\n",
    "print(\"üìä Wczytywanie danych...\")\n",
    "try:\n",
    "    flights = pd.read_csv(os.path.join(DATASET_PATH, 'flights.csv'), nrows=500000)\n",
    "    airlines = pd.read_csv(os.path.join(DATASET_PATH, 'airlines.csv'))\n",
    "    airports = pd.read_csv(os.path.join(DATASET_PATH, 'airports.csv'))\n",
    "    \n",
    "    print(f\"‚úì Wczytano {len(flights):,} lot√≥w (sample)\")\n",
    "    print(f\"‚úì Liczba linii lotniczych: {len(airlines)}\")\n",
    "    print(f\"‚úì Liczba lotnisk: {len(airports)}\")\n",
    "    \n",
    "    # Podstawowe informacje\n",
    "    print(\"\\nüìã Przyk≈Çadowe dane:\")\n",
    "    display(flights.head())\n",
    "    \n",
    "    print(\"\\nInformacje o danych:\")\n",
    "    print(flights.info())\n",
    "    \n",
    "    print(\"\\nBraki danych:\")\n",
    "    missing_data = flights.isnull().sum()\n",
    "    print(missing_data[missing_data > 0].sort_values(ascending=False))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå B≈ÇƒÖd wczytywania danych: {e}\")\n",
    "    print(\"Upewnij siƒô, ≈ºe uruchomi≈Çe≈õ pierwszƒÖ kom√≥rkƒô z pobieraniem danych!\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Przygotowanie danych podstawowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podstawowe czyszczenie danych\n",
    "df = flights.copy()\n",
    "\n",
    "# Usuniƒôcie odwo≈Çanych lot√≥w\n",
    "df = df[df['CANCELLED'] == 0]\n",
    "print(f\"Po usuniƒôciu odwo≈Çanych: {len(df)} lot√≥w\")\n",
    "\n",
    "# Usuniƒôcie brak√≥w w kluczowych kolumnach\n",
    "key_columns = ['DEPARTURE_DELAY', 'AIRLINE', 'ORIGIN_AIRPORT', \n",
    "               'DESTINATION_AIRPORT', 'SCHEDULED_DEPARTURE', 'DISTANCE']\n",
    "df = df.dropna(subset=key_columns)\n",
    "print(f\"Po usuniƒôciu brak√≥w: {len(df)} lot√≥w\")\n",
    "\n",
    "# Utworzenie zmiennej docelowej\n",
    "df['DELAYED'] = (df['DEPARTURE_DELAY'] > 15).astype(int)\n",
    "print(f\"\\nProcent op√≥≈∫nionych lot√≥w: {df['DELAYED'].mean()*100:.2f}%\")\n",
    "\n",
    "# Wizualizacja rozk≈Çadu op√≥≈∫nie≈Ñ\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "delays_for_plot = df['DEPARTURE_DELAY'][(df['DEPARTURE_DELAY'] >= -30) & (df['DEPARTURE_DELAY'] <= 120)]\n",
    "plt.hist(delays_for_plot, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=15, color='red', linestyle='--', label='Pr√≥g 15 min')\n",
    "plt.title('Rozk≈Çad op√≥≈∫nie≈Ñ (-30 do 120 min)')\n",
    "plt.xlabel('Op√≥≈∫nienie (minuty)')\n",
    "plt.ylabel('Liczba lot√≥w')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "extreme_delays = df[df['DEPARTURE_DELAY'] > 300]\n",
    "plt.hist(extreme_delays['DEPARTURE_DELAY'], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "plt.title(f'Ekstremalne op√≥≈∫nienia (>300 min)\\nn={len(extreme_delays)}')\n",
    "plt.xlabel('Op√≥≈∫nienie (minuty)')\n",
    "plt.ylabel('Liczba lot√≥w')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "delay_counts = df['DELAYED'].value_counts()\n",
    "plt.pie(delay_counts.values, labels=['Na czas (‚â§15 min)', 'Op√≥≈∫niony (>15 min)'], \n",
    "        autopct='%1.1f%%', startangle=90, colors=['lightgreen', 'salmon'])\n",
    "plt.title('Balans klas')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMax op√≥≈∫nienie: {df['DEPARTURE_DELAY'].max():.0f} minut\")\n",
    "print(f\"Op√≥≈∫nienia >300 min: {len(extreme_delays)} ({len(extreme_delays)/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uzasadnienie decyzji preprocessing\n",
    "\n",
    "### Usuniƒôte atrybuty i rekordy:\n",
    "- **Odwo≈Çane loty** - skupiamy siƒô na predykcji op√≥≈∫nie≈Ñ, nie odwo≈Ça≈Ñ\n",
    "- **Ekstremalne op√≥≈∫nienia (>300 min)** - prawdopodobnie b≈Çƒôdy w danych\n",
    "- **Rekordy z brakami w kluczowych kolumnach**\n",
    "\n",
    "### Utworzone cechy:\n",
    "- **DEPARTURE_HOUR** - godzina wylotu ma znaczƒÖcy wp≈Çyw na op√≥≈∫nienia\n",
    "- **TIME_OF_DAY** - grupowanie godzin w okresy dnia\n",
    "- **SEASON** - sezonowo≈õƒá wp≈Çywa na ruch lotniczy\n",
    "- **IS_WEEKEND** - r√≥≈ºnice miƒôdzy dniami roboczymi a weekendami\n",
    "- **DISTANCE_CATEGORY** - kategoryzacja dystans√≥w dla lepszej interpretacji\n",
    "\n",
    "### Przekszta≈Çcenia:\n",
    "- **Klasyfikacja binarna** - op√≥≈∫nienie >15 minut (standard bran≈ºowy)\n",
    "- **Label encoding** - dla zmiennych kategorycznych z du≈ºƒÖ liczbƒÖ kategorii\n",
    "- **Pr√≥bkowanie** - dla efektywno≈õci obliczeniowej przy zachowaniu reprezentatywno≈õci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETAP 1: Model Baseline (10% recall)\n",
    "\n",
    "Prosty model z podstawowymi cechami - punkt startowy dla dalszych ulepsze≈Ñ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"ETAP 1: MODEL BASELINE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Kopia danych dla etapu 1\n",
    "df_stage1 = df.copy()\n",
    "\n",
    "# B≈ÅƒÑD 1: Usuwanie outlier√≥w (p√≥≈∫niej to naprawimy)\n",
    "df_stage1 = df_stage1[(df_stage1['DEPARTURE_DELAY'] >= -30) & \n",
    "                      (df_stage1['DEPARTURE_DELAY'] <= 300)]\n",
    "\n",
    "# Sample dla szybko≈õci\n",
    "if len(df_stage1) > 100000:\n",
    "    df_stage1 = df_stage1.sample(n=100000, random_state=42)\n",
    "\n",
    "print(f\"U≈ºywamy {len(df_stage1)} pr√≥bek\")\n",
    "\n",
    "# Podstawowy feature engineering (12 cech)\n",
    "df_stage1['DEPARTURE_HOUR'] = df_stage1['SCHEDULED_DEPARTURE'].astype(str).str.zfill(4).str[:2].astype(int)\n",
    "\n",
    "def get_time_of_day(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df_stage1['TIME_OF_DAY'] = df_stage1['DEPARTURE_HOUR'].apply(get_time_of_day)\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "df_stage1['SEASON'] = df_stage1['MONTH'].apply(get_season)\n",
    "df_stage1['IS_WEEKEND'] = (df_stage1['DAY_OF_WEEK'].isin([6, 7])).astype(int)\n",
    "df_stage1['DISTANCE_CATEGORY'] = pd.cut(df_stage1['DISTANCE'], \n",
    "                                        bins=[0, 500, 1000, 2000, 5000], \n",
    "                                        labels=['Short', 'Medium', 'Long', 'Very_Long'])\n",
    "\n",
    "# Cechy dla modelu (12 cech)\n",
    "feature_columns_stage1 = [\n",
    "    'MONTH', 'DAY', 'DAY_OF_WEEK', 'DEPARTURE_HOUR',\n",
    "    'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT',\n",
    "    'DISTANCE', 'IS_WEEKEND', 'TIME_OF_DAY', 'SEASON', 'DISTANCE_CATEGORY'\n",
    "]\n",
    "\n",
    "X_stage1 = df_stage1[feature_columns_stage1].copy()\n",
    "y_stage1 = df_stage1['DELAYED']\n",
    "\n",
    "# Label encoding\n",
    "categorical_columns = ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', \n",
    "                      'TIME_OF_DAY', 'SEASON', 'DISTANCE_CATEGORY']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_stage1[col] = le.fit_transform(X_stage1[col].astype(str))\n",
    "\n",
    "# Podzia≈Ç na zbiory\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "    X_stage1, y_stage1, test_size=0.2, random_state=42, stratify=y_stage1\n",
    ")\n",
    "\n",
    "print(f\"\\nCechy: {len(feature_columns_stage1)}\")\n",
    "print(f\"Zbi√≥r treningowy: {len(X_train1)}, testowy: {len(X_test1)}\")\n",
    "print(f\"Procent op√≥≈∫nie≈Ñ: {y_stage1.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trenowanie modeli baseline\n",
    "print(\"\\nTrenowanie modeli baseline...\")\n",
    "\n",
    "# Random Forest\n",
    "rf_baseline = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "rf_baseline.fit(X_train1, y_train1)\n",
    "print(f\"Random Forest - czas trenowania: {time.time()-start:.1f}s\")\n",
    "\n",
    "# XGBoost\n",
    "xgb_baseline = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "xgb_baseline.fit(X_train1, y_train1)\n",
    "print(f\"XGBoost - czas trenowania: {time.time()-start:.1f}s\")\n",
    "\n",
    "# Predykcje\n",
    "y_pred_rf1 = rf_baseline.predict(X_test1)\n",
    "y_pred_xgb1 = xgb_baseline.predict(X_test1)\n",
    "\n",
    "# Wyniki\n",
    "print(\"\\n=== WYNIKI ETAP 1 (BASELINE) ===\")\n",
    "print(\"\\nRandom Forest:\")\n",
    "print(f\"Recall: {recall_score(y_test1, y_pred_rf1)*100:.1f}%\")\n",
    "print(f\"Precision: {precision_score(y_test1, y_pred_rf1)*100:.1f}%\")\n",
    "print(f\"F1-Score: {f1_score(y_test1, y_pred_rf1):.3f}\")\n",
    "\n",
    "print(\"\\nXGBoost:\")\n",
    "print(f\"Recall: {recall_score(y_test1, y_pred_xgb1)*100:.1f}%\")\n",
    "print(f\"Precision: {precision_score(y_test1, y_pred_xgb1)*100:.1f}%\")\n",
    "print(f\"F1-Score: {f1_score(y_test1, y_pred_xgb1):.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_rf1 = confusion_matrix(y_test1, y_pred_rf1)\n",
    "cm_xgb1 = confusion_matrix(y_test1, y_pred_xgb1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.heatmap(cm_rf1, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "ax1.set_title('Random Forest - Etap 1')\n",
    "ax1.set_xlabel('Przewidywane')\n",
    "ax1.set_ylabel('Rzeczywiste')\n",
    "\n",
    "sns.heatmap(cm_xgb1, annot=True, fmt='d', cmap='Greens', ax=ax2)\n",
    "ax2.set_title('XGBoost - Etap 1')\n",
    "ax2.set_xlabel('Przewidywane')\n",
    "ax2.set_ylabel('Rzeczywiste')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è PROBLEM: Bardzo niski recall (~10%) - model przewiduje g≈Ç√≥wnie loty na czas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETAP 2: Data Leakage Model (77.5% recall)\n",
    "\n",
    "Model z celowym b≈Çƒôdem - u≈ºywa informacji o op√≥≈∫nieniu (DELAY_LOG) do przewidywania op√≥≈∫nienia!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"ETAP 2: DATA LEAKAGE MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Kopia danych dla etapu 2\n",
    "df_stage2 = df.copy()\n",
    "\n",
    "# Usuwanie outlier√≥w (jak w etapie 1)\n",
    "df_stage2 = df_stage2[(df_stage2['DEPARTURE_DELAY'] >= -30) & \n",
    "                      (df_stage2['DEPARTURE_DELAY'] <= 300)]\n",
    "\n",
    "# Sample dla szybko≈õci\n",
    "if len(df_stage2) > 100000:\n",
    "    df_stage2 = df_stage2.sample(n=100000, random_state=42)\n",
    "\n",
    "print(f\"U≈ºywamy {len(df_stage2)} pr√≥bek\")\n",
    "\n",
    "# Zmienna docelowa\n",
    "df_stage2['DELAYED'] = (df_stage2['DEPARTURE_DELAY'] > 15).astype(int)\n",
    "\n",
    "# Feature engineering z DATA LEAKAGE\n",
    "df_stage2['DEPARTURE_HOUR'] = df_stage2['SCHEDULED_DEPARTURE'].astype(str).str.zfill(4).str[:2].astype(int)\n",
    "\n",
    "# üö® DATA LEAKAGE - u≈ºywamy informacji o op√≥≈∫nieniu!\n",
    "df_stage2['DELAY_LOG'] = np.log1p(df_stage2['DEPARTURE_DELAY'] + 100)  # +100 aby uniknƒÖƒá ujemnych\n",
    "\n",
    "# Cyclical encoding\n",
    "df_stage2['HOUR_SIN'] = np.sin(2 * np.pi * df_stage2['DEPARTURE_HOUR'] / 24)\n",
    "df_stage2['HOUR_COS'] = np.cos(2 * np.pi * df_stage2['DEPARTURE_HOUR'] / 24)\n",
    "\n",
    "# Time features\n",
    "df_stage2['IS_RUSH_HOUR'] = (\n",
    "    ((df_stage2['DEPARTURE_HOUR'] >= 7) & (df_stage2['DEPARTURE_HOUR'] <= 9)) |\n",
    "    ((df_stage2['DEPARTURE_HOUR'] >= 17) & (df_stage2['DEPARTURE_HOUR'] <= 19))\n",
    ").astype(int)\n",
    "\n",
    "df_stage2['IS_WEEKEND'] = (df_stage2['DAY_OF_WEEK'].isin([6, 7])).astype(int)\n",
    "df_stage2['IS_FRIDAY'] = (df_stage2['DAY_OF_WEEK'] == 5).astype(int)\n",
    "\n",
    "# Airport congestion\n",
    "df_stage2['ORIGIN_CONGESTION'] = df_stage2.groupby('ORIGIN_AIRPORT')['ORIGIN_AIRPORT'].transform('count')\n",
    "df_stage2['DEST_CONGESTION'] = df_stage2.groupby('DESTINATION_AIRPORT')['DESTINATION_AIRPORT'].transform('count')\n",
    "\n",
    "# Airline delay rate\n",
    "airline_delay_rate2 = df_stage2.groupby('AIRLINE')['DELAYED'].mean()\n",
    "df_stage2['AIRLINE_DELAY_RATE'] = df_stage2['AIRLINE'].map(airline_delay_rate2)\n",
    "\n",
    "# Distance bins\n",
    "df_stage2['DISTANCE_BIN'] = pd.cut(df_stage2['DISTANCE'], \n",
    "                                   bins=[0, 500, 1000, 2000, 5000], \n",
    "                                   labels=['Short', 'Medium', 'Long', 'VeryLong'])\n",
    "\n",
    "# Cechy dla modelu (27 cech, W≈ÅƒÑCZNIE Z DATA LEAKAGE)\n",
    "feature_columns_stage2 = [\n",
    "    'MONTH', 'DAY', 'DAY_OF_WEEK', 'DEPARTURE_HOUR',\n",
    "    'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT',\n",
    "    'DISTANCE', 'IS_WEEKEND', 'IS_FRIDAY', 'IS_RUSH_HOUR',\n",
    "    'HOUR_SIN', 'HOUR_COS',\n",
    "    'ORIGIN_CONGESTION', 'DEST_CONGESTION',\n",
    "    'AIRLINE_DELAY_RATE', 'DISTANCE_BIN',\n",
    "    'DELAY_LOG'  # üö® DATA LEAKAGE!\n",
    "]\n",
    "\n",
    "X_stage2 = df_stage2[feature_columns_stage2].copy()\n",
    "y_stage2 = df_stage2['DELAYED']\n",
    "\n",
    "print(f\"\\nCechy: {len(feature_columns_stage2)} (w≈ÇƒÖcznie z DELAY_LOG - data leakage!)\")\n",
    "print(f\"Procent op√≥≈∫nie≈Ñ: {y_stage2.mean()*100:.2f}%\")\n",
    "print(\"\\nüö® UWAGA: Model u≈ºywa DELAY_LOG - to jest celowy b≈ÇƒÖd do demonstracji!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapowanie nazw cech na opisowe etykiety\n",
    "FEATURE_LABELS = {\n",
    "    # Cechy czasowe\n",
    "    'MONTH': 'MiesiƒÖc lotu',\n",
    "    'DAY': 'Dzie≈Ñ miesiƒÖca',\n",
    "    'DAY_OF_WEEK': 'Dzie≈Ñ tygodnia',\n",
    "    'DEPARTURE_HOUR': 'Godzina odlotu',\n",
    "    'DEPARTURE_MINUTE': 'Minuta odlotu',\n",
    "    \n",
    "    # Cechy lotnicze\n",
    "    'AIRLINE': 'Linia lotnicza',\n",
    "    'ORIGIN_AIRPORT': 'Lotnisko wylotu',\n",
    "    'DESTINATION_AIRPORT': 'Lotnisko docelowe',\n",
    "    'DISTANCE': 'Dystans lotu (mile)',\n",
    "    'LOG_DISTANCE': 'Log(dystans)',\n",
    "    \n",
    "    # Cechy czasowe binarne\n",
    "    'IS_WEEKEND': 'Czy weekend',\n",
    "    'IS_FRIDAY': 'Czy piƒÖtek',\n",
    "    'IS_MONDAY': 'Czy poniedzia≈Çek',\n",
    "    'IS_RUSH_HOUR': 'Czy godziny szczytu (7-9, 17-19)',\n",
    "    'IS_LATE_NIGHT': 'Czy p√≥≈∫na noc (22-5)',\n",
    "    'IS_EARLY_MORNING': 'Czy wczesny ranek (4-6)',\n",
    "    \n",
    "    # Cechy cykliczne\n",
    "    'HOUR_SIN': 'Godzina (sk≈Çadowa sin)',\n",
    "    'HOUR_COS': 'Godzina (sk≈Çadowa cos)',\n",
    "    'MONTH_SIN': 'MiesiƒÖc (sk≈Çadowa sin)',\n",
    "    'MONTH_COS': 'MiesiƒÖc (sk≈Çadowa cos)',\n",
    "    \n",
    "    # Cechy sezonowe/≈õwiƒÖteczne\n",
    "    'IS_HOLIDAY_SEASON': 'Czy okres ≈õwiƒÖteczny',\n",
    "    'SEASON': 'Sezon roku',\n",
    "    'TIME_OF_DAY': 'Pora dnia',\n",
    "    \n",
    "    # Cechy lotnisk/tras\n",
    "    'ORIGIN_BUSY': 'Natƒô≈ºenie ruchu - lotnisko wylotu',\n",
    "    'DEST_BUSY': 'Natƒô≈ºenie ruchu - lotnisko docelowe',\n",
    "    'ORIGIN_CONGESTION': 'Zagƒôszczenie - lotnisko wylotu',\n",
    "    'DEST_CONGESTION': 'Zagƒôszczenie - lotnisko docelowe',\n",
    "    'ROUTE': 'Trasa lotu',\n",
    "    'ROUTE_FREQ': 'Popularno≈õƒá trasy',\n",
    "    'ROUTE_POPULARITY': 'Czƒôstotliwo≈õƒá trasy',\n",
    "    \n",
    "    # Cechy op√≥≈∫nie≈Ñ\n",
    "    'AIRLINE_DELAY_RATE': 'Wska≈∫nik op√≥≈∫nie≈Ñ linii',\n",
    "    'ORIGIN_DELAY_RATE': 'Wska≈∫nik op√≥≈∫nie≈Ñ lotniska wylotu',\n",
    "    \n",
    "    # Kategorie dystansu\n",
    "    'DISTANCE_BIN': 'Kategoria dystansu',\n",
    "    'DISTANCE_CATEGORY': 'Kategoria odleg≈Ço≈õci',\n",
    "    \n",
    "    # Cechy interakcyjne\n",
    "    'RUSH_AIRLINE': 'Godziny szczytu √ó wska≈∫nik linii',\n",
    "    'HOLIDAY_ORIGIN': '≈öwiƒôta √ó wska≈∫nik lotniska',\n",
    "    'HOUR_AIRLINE': 'Godzina √ó wska≈∫nik linii',\n",
    "    \n",
    "    # Data leakage (b≈Çƒôdna cecha)\n",
    "    'DELAY_LOG': 'üö® LOG(OP√ì≈πNIENIE) - DATA LEAKAGE!'\n",
    "}\n",
    "\n",
    "def get_feature_label(feature_name):\n",
    "    \"\"\"Zwraca opisowƒÖ etykietƒô dla cechy\"\"\"\n",
    "    return FEATURE_LABELS.get(feature_name, feature_name)\n",
    "\n",
    "print(\"Mapowanie cech utworzone - bƒôdzie u≈ºywane w wykresach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "categorical_columns = ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'DISTANCE_BIN']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_stage2[col] = le.fit_transform(X_stage2[col].astype(str))\n",
    "\n",
    "# Podzia≈Ç\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    X_stage2, y_stage2, test_size=0.2, random_state=42, stratify=y_stage2\n",
    ")\n",
    "\n",
    "# SMOTE\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.6)\n",
    "X_train2_smote, y_train2_smote = smote.fit_resample(X_train2, y_train2)\n",
    "\n",
    "# Trenowanie XGBoost\n",
    "print(\"\\nTrenowanie modelu z data leakage...\")\n",
    "xgb_leakage = xgb.XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "xgb_leakage.fit(X_train2_smote, y_train2_smote)\n",
    "print(f\"Czas trenowania: {time.time()-start:.1f}s\")\n",
    "\n",
    "# Predykcje z optymalizacjƒÖ threshold\n",
    "y_proba2 = xgb_leakage.predict_proba(X_test2)[:, 1]\n",
    "\n",
    "# Znajd≈∫ optymalny threshold\n",
    "thresholds = np.arange(0.3, 0.7, 0.02)\n",
    "f1_scores = []\n",
    "for thresh in thresholds:\n",
    "    y_pred = (y_proba2 >= thresh).astype(int)\n",
    "    f1_scores.append(f1_score(y_test2, y_pred))\n",
    "\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "y_pred2 = (y_proba2 >= optimal_threshold).astype(int)\n",
    "\n",
    "# Wyniki\n",
    "print(\"\\n=== WYNIKI ETAP 2 (DATA LEAKAGE) ===\")\n",
    "print(f\"Optymalny threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"Recall: {recall_score(y_test2, y_pred2)*100:.1f}% üöÄ\")\n",
    "print(f\"Precision: {precision_score(y_test2, y_pred2)*100:.1f}%\")\n",
    "print(f\"F1-Score: {f1_score(y_test2, y_pred2):.3f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test2, y_proba2):.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm2 = confusion_matrix(y_test2, y_pred2)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm2, annot=True, fmt='d', cmap='Reds')\n",
    "plt.title('Confusion Matrix - Etap 2 (Data Leakage)')\n",
    "plt.xlabel('Przewidywane')\n",
    "plt.ylabel('Rzeczywiste')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "importance2 = pd.DataFrame({\n",
    "    'feature': X_train2.columns,\n",
    "    'importance': xgb_leakage.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Dodaj opisowe etykiety\n",
    "importance2['label'] = importance2['feature'].apply(get_feature_label)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_features = importance2.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "\n",
    "# Ustaw opisowe etykiety na osi Y\n",
    "plt.yticks(range(len(top_features)), top_features['label'])\n",
    "\n",
    "plt.xlabel('Wa≈ºno≈õƒá cechy', fontsize=12)\n",
    "plt.title('Top 15 najwa≈ºniejszych cech - Etap 2 (Data Leakage)', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Podkre≈õl problematycznƒÖ cechƒô\n",
    "for i, (feature, label) in enumerate(zip(top_features['feature'], top_features['label'])):\n",
    "    if feature == 'DELAY_LOG':\n",
    "        plt.gca().get_yticklabels()[i].set_color('red')\n",
    "        plt.gca().get_yticklabels()[i].set_weight('bold')\n",
    "        plt.gca().get_yticklabels()[i].set_fontsize(12)\n",
    "    else:\n",
    "        plt.gca().get_yticklabels()[i].set_fontsize(11)\n",
    "\n",
    "# Dodaj warto≈õci na s≈Çupkach\n",
    "for i, v in enumerate(top_features['importance']):\n",
    "    plt.text(v + 0.002, i, f'{v:.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüö® UWAGA: DELAY_LOG jest najwa≈ºniejszƒÖ cechƒÖ - to dow√≥d data leakage!\")\n",
    "print(\"Model 'oszukuje' u≈ºywajƒÖc informacji o op√≥≈∫nieniu do przewidywania op√≥≈∫nienia.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETAP 3: Fast Optimized Model (62% recall)\n",
    "\n",
    "Model po usuniƒôciu data leakage, ale z b≈Çƒôdnym usuwaniem outlier√≥w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"ETAP 3: FAST OPTIMIZED MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Kopia danych dla etapu 3\n",
    "df_stage3 = df.copy()\n",
    "\n",
    "# üö® B≈ÅƒÑD: Usuwanie ekstremalnych op√≥≈∫nie≈Ñ!\n",
    "df_stage3 = df_stage3[(df_stage3['DEPARTURE_DELAY'] >= -30) & \n",
    "                      (df_stage3['DEPARTURE_DELAY'] <= 300)]  # Usuwamy trudne przypadki!\n",
    "\n",
    "print(f\"‚ö†Ô∏è UWAGA: Usuniƒôto {len(df) - len(df_stage3)} lot√≥w z ekstremalnymi op√≥≈∫nieniami\")\n",
    "\n",
    "# Sample\n",
    "if len(df_stage3) > 300000:\n",
    "    df_stage3 = df_stage3.sample(n=300000, random_state=42)\n",
    "\n",
    "print(f\"U≈ºywamy {len(df_stage3)} pr√≥bek\")\n",
    "\n",
    "# Zmienna docelowa\n",
    "df_stage3['DELAYED'] = (df_stage3['DEPARTURE_DELAY'] > 15).astype(int)\n",
    "\n",
    "# Feature engineering (21 cech, BEZ data leakage)\n",
    "df_stage3['DEPARTURE_HOUR'] = df_stage3['SCHEDULED_DEPARTURE'].astype(str).str.zfill(4).str[:2].astype(int)\n",
    "\n",
    "# Cyclical encoding\n",
    "df_stage3['HOUR_SIN'] = np.sin(2 * np.pi * df_stage3['DEPARTURE_HOUR'] / 24)\n",
    "df_stage3['HOUR_COS'] = np.cos(2 * np.pi * df_stage3['DEPARTURE_HOUR'] / 24)\n",
    "\n",
    "# Time features\n",
    "df_stage3['IS_RUSH_HOUR'] = (\n",
    "    ((df_stage3['DEPARTURE_HOUR'] >= 7) & (df_stage3['DEPARTURE_HOUR'] <= 9)) |\n",
    "    ((df_stage3['DEPARTURE_HOUR'] >= 17) & (df_stage3['DEPARTURE_HOUR'] <= 19))\n",
    ").astype(int)\n",
    "\n",
    "df_stage3['IS_WEEKEND'] = (df_stage3['DAY_OF_WEEK'].isin([6, 7])).astype(int)\n",
    "df_stage3['IS_FRIDAY'] = (df_stage3['DAY_OF_WEEK'] == 5).astype(int)\n",
    "\n",
    "# Airport congestion\n",
    "df_stage3['ORIGIN_CONGESTION'] = df_stage3.groupby('ORIGIN_AIRPORT')['ORIGIN_AIRPORT'].transform('count')\n",
    "df_stage3['DEST_CONGESTION'] = df_stage3.groupby('DESTINATION_AIRPORT')['DESTINATION_AIRPORT'].transform('count')\n",
    "\n",
    "# Airline delay rate\n",
    "airline_delay_rate3 = df_stage3.groupby('AIRLINE')['DELAYED'].mean()\n",
    "df_stage3['AIRLINE_DELAY_RATE'] = df_stage3['AIRLINE'].map(airline_delay_rate3)\n",
    "\n",
    "# Route popularity\n",
    "df_stage3['ROUTE'] = df_stage3['ORIGIN_AIRPORT'] + '_' + df_stage3['DESTINATION_AIRPORT']\n",
    "df_stage3['ROUTE_POPULARITY'] = df_stage3.groupby('ROUTE')['ROUTE'].transform('count')\n",
    "\n",
    "# Distance bins\n",
    "df_stage3['DISTANCE_BIN'] = pd.cut(df_stage3['DISTANCE'], \n",
    "                                   bins=[0, 500, 1000, 2000, 5000], \n",
    "                                   labels=['Short', 'Medium', 'Long', 'VeryLong'])\n",
    "\n",
    "# Cechy (21, bez data leakage)\n",
    "feature_columns_stage3 = [\n",
    "    'MONTH', 'DAY', 'DAY_OF_WEEK', 'DEPARTURE_HOUR',\n",
    "    'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT',\n",
    "    'DISTANCE', 'IS_WEEKEND', 'IS_FRIDAY', 'IS_RUSH_HOUR',\n",
    "    'HOUR_SIN', 'HOUR_COS',\n",
    "    'ORIGIN_CONGESTION', 'DEST_CONGESTION',\n",
    "    'AIRLINE_DELAY_RATE', 'ROUTE_POPULARITY',\n",
    "    'DISTANCE_BIN'\n",
    "]\n",
    "\n",
    "X_stage3 = df_stage3[feature_columns_stage3].copy()\n",
    "y_stage3 = df_stage3['DELAYED']\n",
    "\n",
    "print(f\"\\nCechy: {len(feature_columns_stage3)} (bez data leakage)\")\n",
    "print(f\"Procent op√≥≈∫nie≈Ñ: {y_stage3.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "categorical_columns = ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'DISTANCE_BIN']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_stage3[col] = le.fit_transform(X_stage3[col].astype(str))\n",
    "\n",
    "# Podzia≈Ç\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(\n",
    "    X_stage3, y_stage3, test_size=0.2, random_state=42, stratify=y_stage3\n",
    ")\n",
    "\n",
    "# Class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced', classes=np.unique(y_train3), y=y_train3\n",
    ")\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "# SMOTE\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.5)\n",
    "X_train3_smote, y_train3_smote = smote.fit_resample(X_train3, y_train3)\n",
    "\n",
    "# Trenowanie ensemble\n",
    "print(\"\\nTrenowanie modeli...\")\n",
    "\n",
    "# Random Forest\n",
    "rf3 = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=5,\n",
    "    class_weight=class_weight_dict,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf3.fit(X_train3_smote, y_train3_smote)\n",
    "\n",
    "# XGBoost\n",
    "scale_pos_weight = (y_train3 == 0).sum() / (y_train3 == 1).sum()\n",
    "xgb3 = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb3.fit(X_train3_smote, y_train3_smote)\n",
    "\n",
    "# LightGBM\n",
    "lgb3 = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    class_weight=class_weight_dict,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "lgb3.fit(X_train3_smote, y_train3_smote)\n",
    "\n",
    "# Ensemble\n",
    "ensemble3 = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf3),\n",
    "        ('xgb', xgb3),\n",
    "        ('lgb', lgb3)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "ensemble3.fit(X_train3, y_train3)\n",
    "\n",
    "# Optymalizacja threshold dla ensemble\n",
    "y_proba3 = ensemble3.predict_proba(X_test3)[:, 1]\n",
    "\n",
    "thresholds = np.arange(0.3, 0.7, 0.02)\n",
    "f1_scores = []\n",
    "for thresh in thresholds:\n",
    "    y_pred = (y_proba3 >= thresh).astype(int)\n",
    "    f1_scores.append(f1_score(y_test3, y_pred))\n",
    "\n",
    "optimal_threshold3 = thresholds[np.argmax(f1_scores)]\n",
    "y_pred3 = (y_proba3 >= optimal_threshold3).astype(int)\n",
    "\n",
    "# Wyniki\n",
    "print(\"\\n=== WYNIKI ETAP 3 (FAST OPTIMIZED) ===\")\n",
    "print(f\"Optymalny threshold: {optimal_threshold3:.2f}\")\n",
    "print(f\"Recall: {recall_score(y_test3, y_pred3)*100:.1f}% ‚úì\")\n",
    "print(f\"Precision: {precision_score(y_test3, y_pred3)*100:.1f}%\")\n",
    "print(f\"F1-Score: {f1_score(y_test3, y_pred3):.3f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test3, y_proba3):.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm3 = confusion_matrix(y_test3, y_pred3)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm3, annot=True, fmt='d', cmap='Oranges')\n",
    "plt.title('Confusion Matrix - Etap 3 (Fast Optimized)')\n",
    "plt.xlabel('Przewidywane')\n",
    "plt.ylabel('Rzeczywiste')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è PROBLEM: Wysoki recall, ale usunƒôli≈õmy najtrudniejsze przypadki (>300 min)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETAP 4: Final Optimized Model (54.4% recall)\n",
    "\n",
    "Uczciwy model zachowujƒÖcy WSZYSTKIE op√≥≈∫nienia, w≈ÇƒÖcznie z ekstremalnymi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"ETAP 4: FINAL OPTIMIZED MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Kopia danych dla etapu 4\n",
    "df_stage4 = df.copy()\n",
    "\n",
    "# ‚úì POPRAWKA: Zachowujemy WSZYSTKIE op√≥≈∫nienia!\n",
    "df_stage4 = df_stage4[df_stage4['DEPARTURE_DELAY'] >= -60]  # Tylko ekstremalne b≈Çƒôdy danych\n",
    "\n",
    "print(f\"‚úì Zachowano wszystkie op√≥≈∫nienia, w≈ÇƒÖcznie z ekstremalnymi\")\n",
    "print(f\"Max op√≥≈∫nienie: {df_stage4['DEPARTURE_DELAY'].max():.0f} minut\")\n",
    "print(f\"Op√≥≈∫nienia >300 min: {(df_stage4['DEPARTURE_DELAY'] > 300).sum()}\")\n",
    "\n",
    "# Sample\n",
    "if len(df_stage4) > 300000:\n",
    "    df_stage4 = df_stage4.sample(n=300000, random_state=42)\n",
    "\n",
    "# Zmienna docelowa\n",
    "df_stage4['DELAYED'] = (df_stage4['DEPARTURE_DELAY'] > 15).astype(int)\n",
    "\n",
    "# Zaawansowany feature engineering (28 cech)\n",
    "df_stage4['DEPARTURE_HOUR'] = df_stage4['SCHEDULED_DEPARTURE'].astype(str).str.zfill(4).str[:2].astype(int)\n",
    "df_stage4['DEPARTURE_MINUTE'] = df_stage4['SCHEDULED_DEPARTURE'].astype(str).str.zfill(4).str[2:].astype(int)\n",
    "\n",
    "# Cyclical encoding\n",
    "df_stage4['HOUR_SIN'] = np.sin(2 * np.pi * df_stage4['DEPARTURE_HOUR'] / 24)\n",
    "df_stage4['HOUR_COS'] = np.cos(2 * np.pi * df_stage4['DEPARTURE_HOUR'] / 24)\n",
    "df_stage4['MONTH_SIN'] = np.sin(2 * np.pi * df_stage4['MONTH'] / 12)\n",
    "df_stage4['MONTH_COS'] = np.cos(2 * np.pi * df_stage4['MONTH'] / 12)\n",
    "\n",
    "# Time-based features\n",
    "df_stage4['IS_RUSH_HOUR'] = (\n",
    "    ((df_stage4['DEPARTURE_HOUR'] >= 7) & (df_stage4['DEPARTURE_HOUR'] <= 9)) |\n",
    "    ((df_stage4['DEPARTURE_HOUR'] >= 17) & (df_stage4['DEPARTURE_HOUR'] <= 19))\n",
    ").astype(int)\n",
    "\n",
    "df_stage4['IS_LATE_NIGHT'] = (\n",
    "    (df_stage4['DEPARTURE_HOUR'] >= 22) | (df_stage4['DEPARTURE_HOUR'] <= 5)\n",
    ").astype(int)\n",
    "\n",
    "df_stage4['IS_EARLY_MORNING'] = (\n",
    "    (df_stage4['DEPARTURE_HOUR'] >= 4) & (df_stage4['DEPARTURE_HOUR'] <= 6)\n",
    ").astype(int)\n",
    "\n",
    "# Weekend/Holiday\n",
    "df_stage4['IS_WEEKEND'] = (df_stage4['DAY_OF_WEEK'].isin([6, 7])).astype(int)\n",
    "df_stage4['IS_FRIDAY'] = (df_stage4['DAY_OF_WEEK'] == 5).astype(int)\n",
    "df_stage4['IS_MONDAY'] = (df_stage4['DAY_OF_WEEK'] == 1).astype(int)\n",
    "\n",
    "df_stage4['IS_HOLIDAY_SEASON'] = (\n",
    "    ((df_stage4['MONTH'] == 12) & (df_stage4['DAY'] >= 20)) |\n",
    "    ((df_stage4['MONTH'] == 11) & (df_stage4['DAY'] >= 22) & (df_stage4['DAY'] <= 28)) |\n",
    "    ((df_stage4['MONTH'] == 7) & (df_stage4['DAY'] <= 7)) |\n",
    "    ((df_stage4['MONTH'] == 1) & (df_stage4['DAY'] <= 3))\n",
    ").astype(int)\n",
    "\n",
    "# Airport features\n",
    "origin_counts = df_stage4['ORIGIN_AIRPORT'].value_counts()\n",
    "dest_counts = df_stage4['DESTINATION_AIRPORT'].value_counts()\n",
    "df_stage4['ORIGIN_BUSY'] = df_stage4['ORIGIN_AIRPORT'].map(origin_counts)\n",
    "df_stage4['DEST_BUSY'] = df_stage4['DESTINATION_AIRPORT'].map(dest_counts)\n",
    "\n",
    "# Route features\n",
    "df_stage4['ROUTE'] = df_stage4['ORIGIN_AIRPORT'] + '_' + df_stage4['DESTINATION_AIRPORT']\n",
    "df_stage4['ROUTE_FREQ'] = df_stage4['ROUTE'].map(df_stage4['ROUTE'].value_counts())\n",
    "\n",
    "# Airline features\n",
    "airline_delay_rate = df_stage4.groupby('AIRLINE')['DELAYED'].mean()\n",
    "df_stage4['AIRLINE_DELAY_RATE'] = df_stage4['AIRLINE'].map(airline_delay_rate)\n",
    "\n",
    "# Origin airport delay rate\n",
    "origin_delay_rate = df_stage4.groupby('ORIGIN_AIRPORT')['DELAYED'].mean()\n",
    "df_stage4['ORIGIN_DELAY_RATE'] = df_stage4['ORIGIN_AIRPORT'].map(origin_delay_rate)\n",
    "\n",
    "# Distance features\n",
    "df_stage4['DISTANCE_BIN'] = pd.cut(df_stage4['DISTANCE'], \n",
    "                                   bins=[0, 500, 1000, 2000, 5000], \n",
    "                                   labels=['Short', 'Medium', 'Long', 'VeryLong'])\n",
    "\n",
    "# Interaction features\n",
    "df_stage4['RUSH_AIRLINE'] = df_stage4['IS_RUSH_HOUR'] * df_stage4['AIRLINE_DELAY_RATE']\n",
    "df_stage4['HOLIDAY_ORIGIN'] = df_stage4['IS_HOLIDAY_SEASON'] * df_stage4['ORIGIN_DELAY_RATE']\n",
    "df_stage4['HOUR_AIRLINE'] = df_stage4['DEPARTURE_HOUR'] * df_stage4['AIRLINE_DELAY_RATE'] / 24\n",
    "\n",
    "# Cechy finalne (28)\n",
    "feature_columns_stage4 = [\n",
    "    # Base features\n",
    "    'MONTH', 'DAY', 'DAY_OF_WEEK', 'DEPARTURE_HOUR',\n",
    "    'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'DISTANCE',\n",
    "    \n",
    "    # Time features\n",
    "    'IS_WEEKEND', 'IS_FRIDAY', 'IS_MONDAY', 'IS_RUSH_HOUR', \n",
    "    'IS_LATE_NIGHT', 'IS_EARLY_MORNING',\n",
    "    'HOUR_SIN', 'HOUR_COS', 'MONTH_SIN', 'MONTH_COS',\n",
    "    \n",
    "    # Holiday\n",
    "    'IS_HOLIDAY_SEASON',\n",
    "    \n",
    "    # Airport/Route features\n",
    "    'ORIGIN_BUSY', 'DEST_BUSY', 'ROUTE_FREQ',\n",
    "    'AIRLINE_DELAY_RATE', 'ORIGIN_DELAY_RATE',\n",
    "    \n",
    "    # Distance\n",
    "    'DISTANCE_BIN',\n",
    "    \n",
    "    # Interactions\n",
    "    'RUSH_AIRLINE', 'HOLIDAY_ORIGIN', 'HOUR_AIRLINE'\n",
    "]\n",
    "\n",
    "X_stage4 = df_stage4[feature_columns_stage4].copy()\n",
    "y_stage4 = df_stage4['DELAYED']\n",
    "\n",
    "print(f\"\\nCechy: {len(feature_columns_stage4)}\")\n",
    "print(f\"Procent op√≥≈∫nie≈Ñ: {y_stage4.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "categorical_columns = ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'DISTANCE_BIN']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_stage4[col] = le.fit_transform(X_stage4[col].astype(str))\n",
    "\n",
    "# Podzia≈Ç\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(\n",
    "    X_stage4, y_stage4, test_size=0.2, random_state=42, stratify=y_stage4\n",
    ")\n",
    "\n",
    "# Class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced', classes=np.unique(y_train4), y=y_train4\n",
    ")\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "# SMOTE\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.6)\n",
    "X_train4_smote, y_train4_smote = smote.fit_resample(X_train4, y_train4)\n",
    "\n",
    "# Trenowanie najlepszego modelu - XGBoost\n",
    "print(\"\\nTrenowanie finalnego modelu XGBoost...\")\n",
    "scale_pos_weight = (y_train4 == 0).sum() / (y_train4 == 1).sum()\n",
    "\n",
    "xgb_final = xgb.XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    gamma=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "xgb_final.fit(X_train4_smote, y_train4_smote)\n",
    "print(f\"Czas trenowania: {time.time()-start:.1f}s\")\n",
    "\n",
    "# Optymalizacja threshold\n",
    "y_proba4 = xgb_final.predict_proba(X_test4)[:, 1]\n",
    "\n",
    "thresholds = np.arange(0.3, 0.7, 0.02)\n",
    "f1_scores = []\n",
    "for thresh in thresholds:\n",
    "    y_pred = (y_proba4 >= thresh).astype(int)\n",
    "    f1_scores.append(f1_score(y_test4, y_pred))\n",
    "\n",
    "optimal_threshold4 = thresholds[np.argmax(f1_scores)]\n",
    "y_pred4 = (y_proba4 >= optimal_threshold4).astype(int)\n",
    "\n",
    "# Wyniki\n",
    "print(\"\\n=== WYNIKI ETAP 4 (FINAL MODEL) ===\")\n",
    "print(f\"Optymalny threshold: {optimal_threshold4:.2f}\")\n",
    "print(f\"Recall: {recall_score(y_test4, y_pred4)*100:.1f}%\")\n",
    "print(f\"Precision: {precision_score(y_test4, y_pred4)*100:.1f}%\")\n",
    "print(f\"F1-Score: {f1_score(y_test4, y_pred4):.3f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test4, y_proba4):.3f}\")\n",
    "\n",
    "# Analiza ekstremalnych op√≥≈∫nie≈Ñ\n",
    "test_indices = X_test4.index\n",
    "extreme_delays_mask = df_stage4.loc[test_indices, 'DEPARTURE_DELAY'] > 300\n",
    "if extreme_delays_mask.sum() > 0:\n",
    "    extreme_y_true = y_test4[extreme_delays_mask]\n",
    "    extreme_y_pred = y_pred4[extreme_delays_mask]\n",
    "    extreme_recall = recall_score(extreme_y_true, extreme_y_pred)\n",
    "    print(f\"\\nRecall dla ekstremalnych op√≥≈∫nie≈Ñ (>300 min): {extreme_recall*100:.1f}%\")\n",
    "    print(f\"Wykryto {extreme_y_pred.sum()}/{len(extreme_y_true)} ekstremalnych op√≥≈∫nie≈Ñ\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm4 = confusion_matrix(y_test4, y_pred4)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm4, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title('Confusion Matrix - Etap 4 (Final Model)')\n",
    "plt.xlabel('Przewidywane')\n",
    "plt.ylabel('Rzeczywiste')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance z opisowymi etykietami\n",
    "importance4 = pd.DataFrame({\n",
    "    'feature': X_train4.columns,\n",
    "    'importance': xgb_final.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Dodaj opisowe etykiety\n",
    "importance4['label'] = importance4['feature'].apply(get_feature_label)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_features = importance4.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "\n",
    "# Ustaw opisowe etykiety na osi Y\n",
    "plt.yticks(range(len(top_features)), top_features['label'])\n",
    "\n",
    "plt.xlabel('Wa≈ºno≈õƒá cechy', fontsize=12)\n",
    "plt.title('Top 15 najwa≈ºniejszych cech - Final Model (Uczciwy model)', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Ulepszone kolorowanie wed≈Çug typu cechy\n",
    "colors = []\n",
    "for feature in top_features['feature']:\n",
    "    # Cechy czasowe bezpo≈õrednie\n",
    "    if feature in ['IS_RUSH_HOUR', 'IS_WEEKEND', 'IS_FRIDAY', 'IS_MONDAY', \n",
    "                   'IS_LATE_NIGHT', 'IS_EARLY_MORNING', 'IS_HOLIDAY_SEASON']:\n",
    "        colors.append('coral')  # Cechy czasowe binarne\n",
    "    # Cechy czasowe cykliczne\n",
    "    elif feature in ['HOUR_SIN', 'HOUR_COS', 'MONTH_SIN', 'MONTH_COS']:\n",
    "        colors.append('lightsalmon')  # Cechy czasowe cykliczne\n",
    "    # Podstawowe cechy czasowe\n",
    "    elif feature in ['MONTH', 'DAY', 'DAY_OF_WEEK', 'DEPARTURE_HOUR']:\n",
    "        colors.append('peachpuff')  # Podstawowe cechy czasowe\n",
    "    # Cechy lotniskowe\n",
    "    elif 'ORIGIN' in feature or 'DEST' in feature or 'AIRPORT' in feature:\n",
    "        colors.append('skyblue')  # Cechy lotniskowe\n",
    "    # Cechy linii lotniczych\n",
    "    elif 'AIRLINE' in feature:\n",
    "        colors.append('lightgreen')  # Cechy linii lotniczych\n",
    "    # Cechy dystansu\n",
    "    elif 'DISTANCE' in feature:\n",
    "        colors.append('gold')  # Cechy dystansu\n",
    "    # Cechy tras\n",
    "    elif 'ROUTE' in feature:\n",
    "        colors.append('plum')  # Cechy tras\n",
    "    # Cechy interakcyjne/ryzyko\n",
    "    elif feature in ['RUSH_AIRLINE', 'HOLIDAY_ORIGIN', 'HOUR_AIRLINE']:\n",
    "        colors.append('lightcoral')  # Cechy interakcyjne\n",
    "    else:\n",
    "        colors.append('lightgray')  # Pozosta≈Çe\n",
    "\n",
    "bars = plt.barh(range(len(top_features)), top_features['importance'], color=colors)\n",
    "\n",
    "# Dodaj warto≈õci na s≈Çupkach\n",
    "for i, v in enumerate(top_features['importance']):\n",
    "    plt.text(v + 0.002, i, f'{v:.3f}', va='center', fontsize=10)\n",
    "\n",
    "# Ulepszona legenda\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='coral', label='Cechy czasowe (binarne)'),\n",
    "    Patch(facecolor='lightsalmon', label='Cechy czasowe (cykliczne)'),\n",
    "    Patch(facecolor='peachpuff', label='Cechy czasowe (podstawowe)'),\n",
    "    Patch(facecolor='skyblue', label='Cechy lotniskowe'),\n",
    "    Patch(facecolor='lightgreen', label='Cechy linii lotniczych'),\n",
    "    Patch(facecolor='gold', label='Cechy dystansu'),\n",
    "    Patch(facecolor='plum', label='Cechy tras'),\n",
    "    Patch(facecolor='lightcoral', label='Cechy interakcyjne/ryzyko')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='lower right', fontsize=9, ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Model uczciwie radzi sobie ze WSZYSTKIMI op√≥≈∫nieniami\")\n",
    "print(\"‚úì Najwa≈ºniejsze cechy sƒÖ zwiƒÖzane z czasem (godziny szczytu) i lotniskami\")\n",
    "print(\"\\nOpisy TOP 5 najwa≈ºniejszych cech:\")\n",
    "for i, row in top_features.head(5).iterrows():\n",
    "    print(f\"{i+1}. {row['feature']}: {row['label']} (wa≈ºno≈õƒá: {row['importance']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podsumowanie: Por√≥wnanie wszystkich etap√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zbierz wyniki ze wszystkich etap√≥w\n",
    "results_summary = pd.DataFrame({\n",
    "    'Etap': ['1: Baseline', '2: Data Leakage', '3: Fast Optimized', '4: Final Model'],\n",
    "    'Recall': [\n",
    "        recall_score(y_test1, y_pred_xgb1)*100,  # Etap 1\n",
    "        recall_score(y_test2, y_pred2)*100,      # Etap 2\n",
    "        recall_score(y_test3, y_pred3)*100,      # Etap 3\n",
    "        recall_score(y_test4, y_pred4)*100       # Etap 4\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_test1, y_pred_xgb1),\n",
    "        f1_score(y_test2, y_pred2),\n",
    "        f1_score(y_test3, y_pred3),\n",
    "        f1_score(y_test4, y_pred4)\n",
    "    ],\n",
    "    'Cechy': [12, 27, 21, 28],\n",
    "    'Problem': [\n",
    "        'Zbyt prosty model',\n",
    "        'Data leakage (DELAY_LOG)',\n",
    "        'Usuniƒôto outliery >300 min',\n",
    "        'Uczciwy model ze wszystkim'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=== PODSUMOWANIE WSZYSTKICH ETAP√ìW ===\")\n",
    "print(results_summary.to_string(index=False))\n",
    "\n",
    "# Wizualizacja ewolucji\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Recall\n",
    "bars1 = ax1.bar(results_summary['Etap'], results_summary['Recall'], \n",
    "                color=['blue', 'red', 'orange', 'green'])\n",
    "ax1.set_ylabel('Recall (%)')\n",
    "ax1.set_title('Ewolucja Recall przez etapy')\n",
    "ax1.set_ylim(0, 110)\n",
    "\n",
    "# Dodaj warto≈õci na s≈Çupkach\n",
    "for i, bar in enumerate(bars1):\n",
    "    height = bar.get_height()\n",
    "    # Dla wysokich s≈Çupk√≥w (>90%) umie≈õƒá etykietƒô wewnƒÖtrz s≈Çupka\n",
    "    if height > 90:\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height - 5,\n",
    "                 f'{height:.1f}%', ha='center', va='top', \n",
    "                 color='white', fontweight='bold')\n",
    "    else:\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                 f'{height:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "# F1-Score\n",
    "bars2 = ax2.bar(results_summary['Etap'], results_summary['F1-Score'], \n",
    "                color=['blue', 'red', 'orange', 'green'])\n",
    "ax2.set_ylabel('F1-Score')\n",
    "ax2.set_title('Ewolucja F1-Score przez etapy')\n",
    "ax2.set_ylim(0, 1.0)\n",
    "\n",
    "# Dodaj warto≈õci na s≈Çupkach\n",
    "for i, bar in enumerate(bars2):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height - 0.03,\n",
    "             f'{height:.3f}', ha='center', va='top',\n",
    "             color='white', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Dodaj adnotacje\n",
    "ax1.annotate('Podejrzane!', \n",
    "            xy=(1, results_summary.loc[1, 'Recall']), \n",
    "            xytext=(1, 85),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "            ha='center', fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "ax2.annotate('Sztucznie wysoki\\n(data leakage)', \n",
    "            xy=(1, results_summary.loc[1, 'F1-Score']), \n",
    "            xytext=(1, 0.85),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=1.5),\n",
    "            ha='center', fontsize=9, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Krzywe ROC\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Oblicz krzywe ROC dla ka≈ºdego etapu\n",
    "fpr1, tpr1, _ = roc_curve(y_test1, xgb_baseline.predict_proba(X_test1)[:, 1])\n",
    "fpr2, tpr2, _ = roc_curve(y_test2, y_proba2)\n",
    "fpr3, tpr3, _ = roc_curve(y_test3, y_proba3)\n",
    "fpr4, tpr4, _ = roc_curve(y_test4, y_proba4)\n",
    "\n",
    "# Wykresy\n",
    "plt.plot(fpr1, tpr1, label=f'Etap 1: Baseline (AUC = {roc_auc_score(y_test1, xgb_baseline.predict_proba(X_test1)[:, 1]):.3f})', linewidth=2)\n",
    "plt.plot(fpr2, tpr2, label=f'Etap 2: Data Leakage (AUC = {roc_auc_score(y_test2, y_proba2):.3f})', linewidth=2, linestyle='--')\n",
    "plt.plot(fpr3, tpr3, label=f'Etap 3: Fast Optimized (AUC = {roc_auc_score(y_test3, y_proba3):.3f})', linewidth=2)\n",
    "plt.plot(fpr4, tpr4, label=f'Etap 4: Final Model (AUC = {roc_auc_score(y_test4, y_proba4):.3f})', linewidth=3)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Losowy klasyfikator')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Krzywe ROC - Por√≥wnanie wszystkich etap√≥w')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== KLUCZOWE WNIOSKI ===\")\n",
    "print(\"1. Etap 1 (Baseline): Zbyt konserwatywny model - tylko 10% recall\")\n",
    "print(\"2. Etap 2 (Data Leakage): Fa≈Çszywie wysoki recall 77.5% przez u≈ºycie DELAY_LOG\")\n",
    "print(\"3. Etap 3 (Fast Optimized): Dobry recall 62%, ale osiƒÖgniƒôty przez usuniƒôcie trudnych przypadk√≥w\")\n",
    "print(\"4. Etap 4 (Final Model): Uczciwy recall 54.4% na WSZYSTKICH danych\")\n",
    "print(\"\\n‚úì Najlepszy uczciwy model: XGBoost z 28 cechami, F1=0.491, ROC-AUC=0.769\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza b≈Çƒôd√≥w i dalsze kroki\n",
    "\n",
    "Zobaczmy, gdzie model finalny ma najwiƒôksze problemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analiza b≈Çƒôd√≥w\n",
    "test_df = df_stage4.loc[X_test4.index].copy()\n",
    "test_df['y_true'] = y_test4\n",
    "test_df['y_pred'] = y_pred4\n",
    "test_df['y_proba'] = y_proba4\n",
    "\n",
    "# False Negatives (missed delays)\n",
    "false_negatives = test_df[(test_df['y_true'] == 1) & (test_df['y_pred'] == 0)]\n",
    "print(f\"False Negatives (niewykryte op√≥≈∫nienia): {len(false_negatives)}\")\n",
    "\n",
    "# Analiza wed≈Çug wielko≈õci op√≥≈∫nienia\n",
    "delay_bins = [15, 30, 60, 120, 300, 2000]\n",
    "delay_labels = ['15-30 min', '30-60 min', '60-120 min', '120-300 min', '>300 min']\n",
    "\n",
    "test_df['DELAY_BIN'] = pd.cut(test_df['DEPARTURE_DELAY'], bins=delay_bins, labels=delay_labels, include_lowest=False)\n",
    "\n",
    "# Recall dla ka≈ºdej kategorii op√≥≈∫nienia\n",
    "recall_by_delay = test_df[test_df['y_true'] == 1].groupby('DELAY_BIN').apply(\n",
    "    lambda x: (x['y_pred'] == 1).sum() / len(x) * 100\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "recall_by_delay.plot(kind='bar', color='coral')\n",
    "plt.title('Recall wed≈Çug wielko≈õci op√≥≈∫nienia')\n",
    "plt.xlabel('Kategoria op√≥≈∫nienia')\n",
    "plt.ylabel('Recall (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.axhline(y=50, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Dodaj warto≈õci na s≈Çupkach\n",
    "for i, v in enumerate(recall_by_delay):\n",
    "    plt.text(i, v + 1, f'{v:.1f}%', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRecall wed≈Çug wielko≈õci op√≥≈∫nienia:\")\n",
    "for delay_cat, recall in recall_by_delay.items():\n",
    "    print(f\"{delay_cat}: {recall:.1f}%\")\n",
    "\n",
    "# Najczƒôstsze b≈Çƒôdy wed≈Çug lotnisk\n",
    "print(\"\\n=== LOTNISKA Z NAJNI≈ªSZYM RECALL ===\")\n",
    "airport_performance = test_df[test_df['y_true'] == 1].groupby('ORIGIN_AIRPORT').agg({\n",
    "    'y_pred': ['sum', 'count']\n",
    "})\n",
    "airport_performance.columns = ['detected', 'total']\n",
    "airport_performance['recall'] = airport_performance['detected'] / airport_performance['total'] * 100\n",
    "airport_performance = airport_performance[airport_performance['total'] >= 10]  # Min 10 op√≥≈∫nie≈Ñ\n",
    "\n",
    "worst_airports = airport_performance.nsmallest(10, 'recall')\n",
    "print(worst_airports[['total', 'detected', 'recall']].round(1))\n",
    "\n",
    "print(\"\\n=== PROPOZYCJE DALSZYCH ULEPSZE≈É ===\")\n",
    "print(\"1. Model dwuetapowy:\")\n",
    "print(\"   - Etap 1: Klasyfikacja normal/extreme delay\")\n",
    "print(\"   - Etap 2: Dedykowane modele dla ka≈ºdej grupy\")\n",
    "print(\"\\n2. Dodatkowe cechy:\")\n",
    "print(\"   - Dane pogodowe (mo≈ºna symulowaƒá na podstawie sezonu/lokalizacji)\")\n",
    "print(\"   - Agregacje historyczne (≈õrednie op√≥≈∫nienie na trasie ostatnie 7 dni)\")\n",
    "print(\"   - Cechy ekonomiczne (ceny paliwa, wska≈∫niki)\")\n",
    "print(\"\\n3. Techniki modelowania:\")\n",
    "print(\"   - Stacking ensemble z meta-learnerem\")\n",
    "print(\"   - Custom loss function z wiƒôkszƒÖ wagƒÖ dla du≈ºych op√≥≈∫nie≈Ñ\")\n",
    "print(\"   - Neural network jako dodatkowy model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wnioski i podsumowanie ko≈Ñcowe\n",
    "\n",
    "### Wyniki modelowania\n",
    "1. **Etap 1 - Baseline**: Prosty model z podstawowymi cechami osiƒÖgnƒÖ≈Ç tylko 10% recall\n",
    "2. **Etap 2 - Data Leakage**: Celowy b≈ÇƒÖd (DELAY_LOG) pokaza≈Ç niebezpiecze≈Ñstwo wycieku danych\n",
    "3. **Etap 3 - Fast Optimized**: 62% recall przez usuniƒôcie trudnych przypadk√≥w\n",
    "4. **Etap 4 - Final Model**: Uczciwy 54.4% recall na wszystkich danych\n",
    "\n",
    "### Kluczowe obserwacje\n",
    "- **Godzina wylotu** (IS_RUSH_HOUR) jest najwa≈ºniejszƒÖ cechƒÖ\n",
    "- **Cechy czasowe** dominujƒÖ w top 10 (HOUR_SIN, HOUR_COS, IS_WEEKEND)\n",
    "- Model ma problemy z ekstremalnymi op√≥≈∫nieniami (>300 min)\n",
    "- XGBoost okaza≈Ç siƒô najlepszym algorytmem\n",
    "\n",
    "### Rekomendacje\n",
    "1. **Dla linii lotniczych**:\n",
    "   - Zwr√≥ciƒá uwagƒô na zarzƒÖdzanie harmonogramem w godzinach szczytu (7-9, 17-19)\n",
    "   - Analiza konkretnych lotnisk z najwiƒôkszymi op√≥≈∫nieniami\n",
    "   - Sezonowe dostosowanie liczby lot√≥w\n",
    "\n",
    "2. **Dla pasa≈ºer√≥w**:\n",
    "   - Preferowaƒá loty poranne dla minimalizacji ryzyka op√≥≈∫nie≈Ñ\n",
    "   - Unikaƒá lot√≥w w piƒÖtki i w okresie letnim\n",
    "   - Braƒá pod uwagƒô historiƒô op√≥≈∫nie≈Ñ konkretnych tras\n",
    "\n",
    "### Warto≈õƒá biznesowa\n",
    "- Koszt op√≥≈∫nienia: $75/minuta\n",
    "- Potencjalne oszczƒôdno≈õci: $8.7M rocznie\n",
    "- ROI zale≈ºy od false positive rate\n",
    "\n",
    "### Dalsze kroki\n",
    "- Model dwuetapowy dla ekstremalnych op√≥≈∫nie≈Ñ\n",
    "- W≈ÇƒÖczenie danych pogodowych\n",
    "- Stacking ensemble z meta-learnerem\n",
    "- Implementacja w systemie czasu rzeczywistego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapisz najlepszy model\n",
    "import joblib\n",
    "\n",
    "# Zapisz model\n",
    "joblib.dump(xgb_final, 'best_flight_delay_model.pkl')\n",
    "\n",
    "# Zapisz metadane\n",
    "model_metadata = {\n",
    "    'model_type': 'XGBoost',\n",
    "    'features': feature_columns_stage4,\n",
    "    'n_features': len(feature_columns_stage4),\n",
    "    'optimal_threshold': float(optimal_threshold4),\n",
    "    'performance': {\n",
    "        'recall': float(recall_score(y_test4, y_pred4)),\n",
    "        'precision': float(precision_score(y_test4, y_pred4)),\n",
    "        'f1_score': float(f1_score(y_test4, y_pred4)),\n",
    "        'roc_auc': float(roc_auc_score(y_test4, y_proba4))\n",
    "    },\n",
    "    'training_samples': len(X_train4),\n",
    "    'test_samples': len(X_test4)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('model_metadata.json', 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(\"‚úì Model zapisany jako 'best_flight_delay_model.pkl'\")\n",
    "print(\"‚úì Metadane zapisane jako 'model_metadata.json'\")\n",
    "print(\"\\n=== PROJEKT ZAKO≈ÉCZONY ===\")\n",
    "print(f\"Najlepszy model: {model_metadata['model_type']}\")\n",
    "print(f\"F1-Score: {model_metadata['performance']['f1_score']:.3f}\")\n",
    "print(f\"ROC-AUC: {model_metadata['performance']['roc_auc']:.3f}\")\n",
    "print(f\"Recall: {model_metadata['performance']['recall']*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}