{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# üìä Analiza ML - Przewidywanie op√≥≈∫nie≈Ñ lot√≥w\n\n## üöÄ Szybki start\n\n### Wymagania:\n```bash\npip install pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm imbalanced-learn kagglehub joblib\n```\n\n### Automatyczne pobieranie danych:\n- Notebook automatycznie pobierze dane z Kaggle przy pierwszym uruchomieniu\n- Wymagane: konto Kaggle i token API ([instrukcja](https://github.com/Kaggle/kagglehub))\n- Dataset: [US Flight Delays](https://www.kaggle.com/datasets/usdot/flight-delays)\n\n### Alternatywnie - pobierz dane rƒôcznie:\n```python\nimport kagglehub\nkagglehub.dataset_download(\"usdot/flight-delays\")\n```\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Automatyczne pobieranie danych - uruchom tƒô kom√≥rkƒô najpierw!\nimport os\nimport sys\n\n# Sprawd≈∫ czy mamy kagglehub\ntry:\n    import kagglehub\nexcept ImportError:\n    print(\"Instalujƒô kagglehub...\")\n    import subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"kagglehub\"])\n    import kagglehub\n\n# Pr√≥buj znale≈∫ƒá dane lokalnie najpierw\npossible_paths = [\n    'data',  # lokalny folder\n    '../data',  # folder wy≈ºej\n    os.path.join(os.getcwd(), 'data'),\n]\n\nDATASET_PATH = None\n\n# Sprawd≈∫ lokalne foldery\nfor path in possible_paths:\n    if os.path.exists(path) and os.path.exists(os.path.join(path, 'flights.csv')):\n        DATASET_PATH = path\n        print(f\"‚úì Znaleziono dane lokalnie w: {DATASET_PATH}\")\n        break\n\n# Je≈õli nie znaleziono lokalnie, pobierz z Kaggle\nif DATASET_PATH is None:\n    print(\"üì• Pobieram dane z Kaggle (to mo≈ºe chwilƒô potrwaƒá za pierwszym razem)...\")\n    try:\n        DATASET_PATH = kagglehub.dataset_download(\"usdot/flight-delays\")\n        print(f\"‚úì Dane pobrane do: {DATASET_PATH}\")\n    except Exception as e:\n        print(f\"‚ùå B≈ÇƒÖd pobierania: {e}\")\n        print(\"\\nüîß RozwiƒÖzania:\")\n        print(\"1. Upewnij siƒô, ≈ºe masz konto Kaggle i skonfigurowany token API\")\n        print(\"   - Zaloguj siƒô na https://www.kaggle.com\")\n        print(\"   - Id≈∫ do Account -> Create New API Token\")\n        print(\"   - Zapisz plik kaggle.json w ~/.kaggle/ (Linux/Mac) lub C:\\\\Users\\\\[username]\\\\.kaggle\\\\ (Windows)\")\n        print(\"\\n2. Lub pobierz dane rƒôcznie:\")\n        print(\"   - https://www.kaggle.com/datasets/usdot/flight-delays\")\n        print(\"   - Rozpakuj do folderu 'data' obok tego notebooka\")\n        raise\n\n# Sprawd≈∫ czy pliki istniejƒÖ\nrequired_files = ['flights.csv', 'airlines.csv', 'airports.csv']\nmissing_files = []\nfor file in required_files:\n    if not os.path.exists(os.path.join(DATASET_PATH, file)):\n        missing_files.append(file)\n\nif missing_files:\n    print(f\"‚ùå Brakuje plik√≥w: {missing_files}\")\n    raise FileNotFoundError(f\"Nie znaleziono wymaganych plik√≥w: {missing_files}\")\nelse:\n    print(\"‚úÖ Wszystkie pliki danych sƒÖ dostƒôpne!\")\n    print(f\"üìÅ Lokalizacja: {os.path.abspath(DATASET_PATH)}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "import kagglehub\n\n# Download dataset and get the path dynamically\ndataset_path = kagglehub.dataset_download(\"usdot/flight-delays\")\nDATASET_PATH = dataset_path\n\nprint(\"Path to dataset files:\", DATASET_PATH)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Import bibliotek\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.metrics import (\n    accuracy_score, roc_auc_score, f1_score, recall_score, \n    precision_score, confusion_matrix, classification_report, roc_curve\n)\nfrom sklearn.utils import class_weight\nfrom imblearn.over_sampling import SMOTE\nimport warnings\nimport time\nwarnings.filterwarnings('ignore')\n\n# Konfiguracja\nplt.style.use('seaborn-v0_8-darkgrid')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\n\nprint(\"Biblioteki za≈Çadowane pomy≈õlnie!\")\n\n# Wczytanie danych (u≈ºywamy DATASET_PATH z poprzedniej kom√≥rki)\nprint(\"\\nüìä Wczytywanie danych...\")\ntry:\n    flights = pd.read_csv(os.path.join(DATASET_PATH, 'flights.csv'), nrows=500000)\n    airlines = pd.read_csv(os.path.join(DATASET_PATH, 'airlines.csv'))\n    airports = pd.read_csv(os.path.join(DATASET_PATH, 'airports.csv'))\n    \n    print(f\"‚úì Wczytano {len(flights):,} lot√≥w (sample)\")\n    print(f\"‚úì Liczba linii lotniczych: {len(airlines)}\")\n    print(f\"‚úì Liczba lotnisk: {len(airports)}\")\n    \n    # Podstawowe informacje\n    print(\"\\nüìã Przyk≈Çadowe dane:\")\n    display(flights.head())\n    \nexcept Exception as e:\n    print(f\"‚ùå B≈ÇƒÖd wczytywania danych: {e}\")\n    print(\"Upewnij siƒô, ≈ºe uruchomi≈Çe≈õ pierwszƒÖ kom√≥rkƒô z pobieraniem danych!\")\n    raise"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Wczytanie danych (u≈ºywamy sample dla szybko≈õci)\nprint(\"Wczytywanie danych...\")\nflights = pd.read_csv(os.path.join(DATASET_PATH, 'flights.csv'), nrows=500000)\nairlines = pd.read_csv(os.path.join(DATASET_PATH, 'airlines.csv'))\nairports = pd.read_csv(os.path.join(DATASET_PATH, 'airports.csv'))\n\nprint(f\"‚úì Wczytano {len(flights):,} lot√≥w (sample)\")\nprint(f\"‚úì Liczba linii lotniczych: {len(airlines)}\")\nprint(f\"‚úì Liczba lotnisk: {len(airports)}\")\n\n# Podstawowe informacje\nprint(\"\\nüìã Przyk≈Çadowe dane:\")\nflights.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie danych podstawowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podstawowe czyszczenie danych\n",
    "df = flights.copy()\n",
    "\n",
    "# Usuniƒôcie odwo≈Çanych lot√≥w\n",
    "df = df[df['CANCELLED'] == 0]\n",
    "print(f\"Po usuniƒôciu odwo≈Çanych: {len(df)} lot√≥w\")\n",
    "\n",
    "# Usuniƒôcie brak√≥w w kluczowych kolumnach\n",
    "key_columns = ['DEPARTURE_DELAY', 'AIRLINE', 'ORIGIN_AIRPORT', \n",
    "               'DESTINATION_AIRPORT', 'SCHEDULED_DEPARTURE', 'DISTANCE']\n",
    "df = df.dropna(subset=key_columns)\n",
    "print(f\"Po usuniƒôciu brak√≥w: {len(df)} lot√≥w\")\n",
    "\n",
    "# Utworzenie zmiennej docelowej\n",
    "df['DELAYED'] = (df['DEPARTURE_DELAY'] > 15).astype(int)\n",
    "print(f\"\\nProcent op√≥≈∫nionych lot√≥w: {df['DELAYED'].mean()*100:.2f}%\")\n",
    "\n",
    "# Wizualizacja rozk≈Çadu op√≥≈∫nie≈Ñ\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "delays_for_plot = df['DEPARTURE_DELAY'][(df['DEPARTURE_DELAY'] >= -30) & (df['DEPARTURE_DELAY'] <= 120)]\n",
    "plt.hist(delays_for_plot, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=15, color='red', linestyle='--', label='Pr√≥g 15 min')\n",
    "plt.title('Rozk≈Çad op√≥≈∫nie≈Ñ (-30 do 120 min)')\n",
    "plt.xlabel('Op√≥≈∫nienie (minuty)')\n",
    "plt.ylabel('Liczba lot√≥w')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "extreme_delays = df[df['DEPARTURE_DELAY'] > 300]\n",
    "plt.hist(extreme_delays['DEPARTURE_DELAY'], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "plt.title(f'Ekstremalne op√≥≈∫nienia (>300 min)\\nn={len(extreme_delays)}')\n",
    "plt.xlabel('Op√≥≈∫nienie (minuty)')\n",
    "plt.ylabel('Liczba lot√≥w')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "delay_counts = df['DELAYED'].value_counts()\n",
    "plt.pie(delay_counts.values, labels=['Na czas (‚â§15 min)', 'Op√≥≈∫niony (>15 min)'], \n",
    "        autopct='%1.1f%%', startangle=90, colors=['lightgreen', 'salmon'])\n",
    "plt.title('Balans klas')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMax op√≥≈∫nienie: {df['DEPARTURE_DELAY'].max():.0f} minut\")\n",
    "print(f\"Op√≥≈∫nienia >300 min: {len(extreme_delays)} ({len(extreme_delays)/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETAP 1: Model Baseline (10% recall)\n",
    "\n",
    "Prosty model z podstawowymi cechami - punkt startowy dla dalszych ulepsze≈Ñ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"ETAP 1: MODEL BASELINE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Kopia danych dla etapu 1\n",
    "df_stage1 = df.copy()\n",
    "\n",
    "# B≈ÅƒÑD 1: Usuwanie outlier√≥w (p√≥≈∫niej to naprawimy)\n",
    "df_stage1 = df_stage1[(df_stage1['DEPARTURE_DELAY'] >= -30) & \n",
    "                      (df_stage1['DEPARTURE_DELAY'] <= 300)]\n",
    "\n",
    "# Sample dla szybko≈õci\n",
    "if len(df_stage1) > 100000:\n",
    "    df_stage1 = df_stage1.sample(n=100000, random_state=42)\n",
    "\n",
    "print(f\"U≈ºywamy {len(df_stage1)} pr√≥bek\")\n",
    "\n",
    "# Podstawowy feature engineering (12 cech)\n",
    "df_stage1['DEPARTURE_HOUR'] = df_stage1['SCHEDULED_DEPARTURE'].astype(str).str.zfill(4).str[:2].astype(int)\n",
    "\n",
    "def get_time_of_day(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df_stage1['TIME_OF_DAY'] = df_stage1['DEPARTURE_HOUR'].apply(get_time_of_day)\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "df_stage1['SEASON'] = df_stage1['MONTH'].apply(get_season)\n",
    "df_stage1['IS_WEEKEND'] = (df_stage1['DAY_OF_WEEK'].isin([6, 7])).astype(int)\n",
    "df_stage1['DISTANCE_CATEGORY'] = pd.cut(df_stage1['DISTANCE'], \n",
    "                                        bins=[0, 500, 1000, 2000, 5000], \n",
    "                                        labels=['Short', 'Medium', 'Long', 'Very_Long'])\n",
    "\n",
    "# Cechy dla modelu (12 cech)\n",
    "feature_columns_stage1 = [\n",
    "    'MONTH', 'DAY', 'DAY_OF_WEEK', 'DEPARTURE_HOUR',\n",
    "    'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT',\n",
    "    'DISTANCE', 'IS_WEEKEND', 'TIME_OF_DAY', 'SEASON', 'DISTANCE_CATEGORY'\n",
    "]\n",
    "\n",
    "X_stage1 = df_stage1[feature_columns_stage1].copy()\n",
    "y_stage1 = df_stage1['DELAYED']\n",
    "\n",
    "# Label encoding\n",
    "categorical_columns = ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', \n",
    "                      'TIME_OF_DAY', 'SEASON', 'DISTANCE_CATEGORY']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_stage1[col] = le.fit_transform(X_stage1[col].astype(str))\n",
    "\n",
    "# Podzia≈Ç na zbiory\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "    X_stage1, y_stage1, test_size=0.2, random_state=42, stratify=y_stage1\n",
    ")\n",
    "\n",
    "print(f\"\\nCechy: {len(feature_columns_stage1)}\")\n",
    "print(f\"Zbi√≥r treningowy: {len(X_train1)}, testowy: {len(X_test1)}\")\n",
    "print(f\"Procent op√≥≈∫nie≈Ñ: {y_stage1.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trenowanie modeli baseline\n",
    "print(\"\\nTrenowanie modeli baseline...\")\n",
    "\n",
    "# Random Forest\n",
    "rf_baseline = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "rf_baseline.fit(X_train1, y_train1)\n",
    "print(f\"Random Forest - czas trenowania: {time.time()-start:.1f}s\")\n",
    "\n",
    "# XGBoost\n",
    "xgb_baseline = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "xgb_baseline.fit(X_train1, y_train1)\n",
    "print(f\"XGBoost - czas trenowania: {time.time()-start:.1f}s\")\n",
    "\n",
    "# Predykcje\n",
    "y_pred_rf1 = rf_baseline.predict(X_test1)\n",
    "y_pred_xgb1 = xgb_baseline.predict(X_test1)\n",
    "\n",
    "# Wyniki\n",
    "print(\"\\n=== WYNIKI ETAP 1 (BASELINE) ===\")\n",
    "print(\"\\nRandom Forest:\")\n",
    "print(f\"Recall: {recall_score(y_test1, y_pred_rf1)*100:.1f}%\")\n",
    "print(f\"Precision: {precision_score(y_test1, y_pred_rf1)*100:.1f}%\")\n",
    "print(f\"F1-Score: {f1_score(y_test1, y_pred_rf1):.3f}\")\n",
    "\n",
    "print(\"\\nXGBoost:\")\n",
    "print(f\"Recall: {recall_score(y_test1, y_pred_xgb1)*100:.1f}%\")\n",
    "print(f\"Precision: {precision_score(y_test1, y_pred_xgb1)*100:.1f}%\")\n",
    "print(f\"F1-Score: {f1_score(y_test1, y_pred_xgb1):.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_rf1 = confusion_matrix(y_test1, y_pred_rf1)\n",
    "cm_xgb1 = confusion_matrix(y_test1, y_pred_xgb1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.heatmap(cm_rf1, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "ax1.set_title('Random Forest - Etap 1')\n",
    "ax1.set_xlabel('Przewidywane')\n",
    "ax1.set_ylabel('Rzeczywiste')\n",
    "\n",
    "sns.heatmap(cm_xgb1, annot=True, fmt='d', cmap='Greens', ax=ax2)\n",
    "ax2.set_title('XGBoost - Etap 1')\n",
    "ax2.set_xlabel('Przewidywane')\n",
    "ax2.set_ylabel('Rzeczywiste')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è PROBLEM: Bardzo niski recall (~10%) - model przewiduje g≈Ç√≥wnie loty na czas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Trenowanie modeli baseline\nprint(\"\\nTrenowanie modeli baseline...\")\n\n# Random Forest\nrf_baseline = RandomForestClassifier(\n    n_estimators=50,\n    max_depth=20,\n    min_samples_split=10,\n    min_samples_leaf=5,\n    random_state=42,\n    n_jobs=-1\n)\n\nstart = time.time()\nrf_baseline.fit(X_train1, y_train1)\nprint(f\"Random Forest - czas trenowania: {time.time()-start:.1f}s\")\n\n# XGBoost\nxgb_baseline = xgb.XGBClassifier(\n    n_estimators=100,\n    max_depth=5,\n    learning_rate=0.1,\n    subsample=0.8,\n    random_state=42,\n    n_jobs=-1,\n    use_label_encoder=False,\n    eval_metric='logloss'\n)\n\nstart = time.time()\nxgb_baseline.fit(X_train1, y_train1)\nprint(f\"XGBoost - czas trenowania: {time.time()-start:.1f}s\")\n\n# Predykcje\ny_pred_rf1 = rf_baseline.predict(X_test1)\ny_pred_xgb1 = xgb_baseline.predict(X_test1)\n\n# Wyniki\nprint(\"\\n=== WYNIKI ETAP 1 (BASELINE) ===\")\nprint(\"\\nRandom Forest:\")\nprint(f\"Recall: {recall_score(y_test1, y_pred_rf1)*100:.1f}%\")\nprint(f\"Precision: {precision_score(y_test1, y_pred_rf1)*100:.1f}%\")\nprint(f\"F1-Score: {f1_score(y_test1, y_pred_rf1):.3f}\")\n\nprint(\"\\nXGBoost:\")\nprint(f\"Recall: {recall_score(y_test1, y_pred_xgb1)*100:.1f}%\")\nprint(f\"Precision: {precision_score(y_test1, y_pred_xgb1)*100:.1f}%\")\nprint(f\"F1-Score: {f1_score(y_test1, y_pred_xgb1):.3f}\")\n\n# Confusion matrix\ncm_rf1 = confusion_matrix(y_test1, y_pred_rf1)\ncm_xgb1 = confusion_matrix(y_test1, y_pred_xgb1)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nsns.heatmap(cm_rf1, annot=True, fmt='d', cmap='Blues', ax=ax1)\nax1.set_title('Random Forest - Etap 1')\nax1.set_xlabel('Przewidywane')\nax1.set_ylabel('Rzeczywiste')\n\nsns.heatmap(cm_xgb1, annot=True, fmt='d', cmap='Greens', ax=ax2)\nax2.set_title('XGBoost - Etap 1')\nax2.set_xlabel('Przewidywane')\nax2.set_ylabel('Rzeczywiste')\n\nplt.tight_layout()\nplt.show()\n\n# Feature importance dla modelu baseline\nimportance1 = pd.DataFrame({\n    'feature': X_train1.columns,\n    'importance': xgb_baseline.feature_importances_\n}).sort_values('importance', ascending=False)\n\n# Dodaj opisowe etykiety\nimportance1['label'] = importance1['feature'].apply(get_feature_label)\n\nplt.figure(figsize=(12, 8))\ntop_features_baseline = importance1.head(12)  # Wszystkie 12 cech\nplt.barh(range(len(top_features_baseline)), top_features_baseline['importance'])\n\n# Ustaw opisowe etykiety na osi Y\nplt.yticks(range(len(top_features_baseline)), top_features_baseline['label'])\n\nplt.xlabel('Wa≈ºno≈õƒá cechy', fontsize=12)\nplt.title('Wa≈ºno≈õƒá cech - Etap 1 (Baseline Model)', fontsize=14)\nplt.gca().invert_yaxis()\n\n# Dodaj warto≈õci na s≈Çupkach\nfor i, v in enumerate(top_features_baseline['importance']):\n    plt.text(v + 0.002, i, f'{v:.3f}', va='center', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚ö†Ô∏è PROBLEM: Bardzo niski recall (~10%) - model przewiduje g≈Ç√≥wnie loty na czas!\")\nprint(\"\\nNajwa≈ºniejsze cechy w modelu baseline:\")\nfor i, row in top_features_baseline.head(5).iterrows():\n    print(f\"{i+1}. {row['feature']}: {row['label']} (wa≈ºno≈õƒá: {row['importance']:.3f})\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Mapowanie nazw cech na opisowe etykiety\nFEATURE_LABELS = {\n    # Cechy czasowe\n    'MONTH': 'MiesiƒÖc lotu',\n    'DAY': 'Dzie≈Ñ miesiƒÖca',\n    'DAY_OF_WEEK': 'Dzie≈Ñ tygodnia',\n    'DEPARTURE_HOUR': 'Godzina odlotu',\n    'DEPARTURE_MINUTE': 'Minuta odlotu',\n    \n    # Cechy lotnicze\n    'AIRLINE': 'Linia lotnicza',\n    'ORIGIN_AIRPORT': 'Lotnisko wylotu',\n    'DESTINATION_AIRPORT': 'Lotnisko docelowe',\n    'DISTANCE': 'Dystans lotu (mile)',\n    'LOG_DISTANCE': 'Log(dystans)',\n    \n    # Cechy czasowe binarne\n    'IS_WEEKEND': 'Czy weekend',\n    'IS_FRIDAY': 'Czy piƒÖtek',\n    'IS_MONDAY': 'Czy poniedzia≈Çek',\n    'IS_RUSH_HOUR': 'Czy godziny szczytu (7-9, 17-19)',\n    'IS_LATE_NIGHT': 'Czy p√≥≈∫na noc (22-5)',\n    'IS_EARLY_MORNING': 'Czy wczesny ranek (4-6)',\n    \n    # Cechy cykliczne\n    'HOUR_SIN': 'Godzina (sk≈Çadowa sin)',\n    'HOUR_COS': 'Godzina (sk≈Çadowa cos)',\n    'MONTH_SIN': 'MiesiƒÖc (sk≈Çadowa sin)',\n    'MONTH_COS': 'MiesiƒÖc (sk≈Çadowa cos)',\n    \n    # Cechy sezonowe/≈õwiƒÖteczne\n    'IS_HOLIDAY_SEASON': 'Czy okres ≈õwiƒÖteczny',\n    'SEASON': 'Sezon roku',\n    'TIME_OF_DAY': 'Pora dnia',\n    \n    # Cechy lotnisk/tras\n    'ORIGIN_BUSY': 'Natƒô≈ºenie ruchu - lotnisko wylotu',\n    'DEST_BUSY': 'Natƒô≈ºenie ruchu - lotnisko docelowe',\n    'ORIGIN_CONGESTION': 'Zagƒôszczenie - lotnisko wylotu',\n    'DEST_CONGESTION': 'Zagƒôszczenie - lotnisko docelowe',\n    'ROUTE': 'Trasa lotu',\n    'ROUTE_FREQ': 'Popularno≈õƒá trasy',\n    'ROUTE_POPULARITY': 'Czƒôstotliwo≈õƒá trasy',\n    \n    # Cechy op√≥≈∫nie≈Ñ\n    'AIRLINE_DELAY_RATE': 'Wska≈∫nik op√≥≈∫nie≈Ñ linii',\n    'ORIGIN_DELAY_RATE': 'Wska≈∫nik op√≥≈∫nie≈Ñ lotniska wylotu',\n    \n    # Kategorie dystansu\n    'DISTANCE_BIN': 'Kategoria dystansu',\n    'DISTANCE_CATEGORY': 'Kategoria odleg≈Ço≈õci',\n    \n    # Cechy interakcyjne\n    'RUSH_AIRLINE': 'Godziny szczytu √ó wska≈∫nik linii',\n    'HOLIDAY_ORIGIN': '≈öwiƒôta √ó wska≈∫nik lotniska',\n    'HOUR_AIRLINE': 'Godzina √ó wska≈∫nik linii',\n    \n    # Data leakage (b≈Çƒôdna cecha)\n    'DELAY_LOG': 'üö® LOG(OP√ì≈πNIENIE) - DATA LEAKAGE!'\n}\n\ndef get_feature_label(feature_name):\n    \"\"\"Zwraca opisowƒÖ etykietƒô dla cechy\"\"\"\n    return FEATURE_LABELS.get(feature_name, feature_name)\n\nprint(\"Mapowanie cech utworzone - bƒôdzie u≈ºywane w wykresach\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Mapowanie nazw cech na opisowe etykiety\nFEATURE_LABELS = {\n    # Cechy czasowe\n    'MONTH': 'MiesiƒÖc lotu',\n    'DAY': 'Dzie≈Ñ miesiƒÖca',\n    'DAY_OF_WEEK': 'Dzie≈Ñ tygodnia',\n    'DEPARTURE_HOUR': 'Godzina odlotu',\n    'DEPARTURE_MINUTE': 'Minuta odlotu',\n    \n    # Cechy lotnicze\n    'AIRLINE': 'Linia lotnicza',\n    'ORIGIN_AIRPORT': 'Lotnisko wylotu',\n    'DESTINATION_AIRPORT': 'Lotnisko docelowe',\n    'DISTANCE': 'Dystans lotu (mile)',\n    'LOG_DISTANCE': 'Log(dystans)',\n    \n    # Cechy czasowe binarne\n    'IS_WEEKEND': 'Czy weekend',\n    'IS_FRIDAY': 'Czy piƒÖtek',\n    'IS_MONDAY': 'Czy poniedzia≈Çek',\n    'IS_RUSH_HOUR': 'Czy godziny szczytu (7-9, 17-19)',\n    'IS_LATE_NIGHT': 'Czy p√≥≈∫na noc (22-5)',\n    'IS_EARLY_MORNING': 'Czy wczesny ranek (4-6)',\n    \n    # Cechy cykliczne\n    'HOUR_SIN': 'Godzina (sk≈Çadowa sin)',\n    'HOUR_COS': 'Godzina (sk≈Çadowa cos)',\n    'MONTH_SIN': 'MiesiƒÖc (sk≈Çadowa sin)',\n    'MONTH_COS': 'MiesiƒÖc (sk≈Çadowa cos)',\n    \n    # Cechy sezonowe/≈õwiƒÖteczne\n    'IS_HOLIDAY_SEASON': 'Czy okres ≈õwiƒÖteczny',\n    'SEASON': 'Sezon roku',\n    'TIME_OF_DAY': 'Pora dnia',\n    \n    # Cechy lotnisk/tras\n    'ORIGIN_BUSY': 'Natƒô≈ºenie ruchu - lotnisko wylotu',\n    'DEST_BUSY': 'Natƒô≈ºenie ruchu - lotnisko docelowe',\n    'ORIGIN_CONGESTION': 'Zagƒôszczenie - lotnisko wylotu',\n    'DEST_CONGESTION': 'Zagƒôszczenie - lotnisko docelowe',\n    'ROUTE': 'Trasa lotu',\n    'ROUTE_FREQ': 'Popularno≈õƒá trasy',\n    'ROUTE_POPULARITY': 'Czƒôstotliwo≈õƒá trasy',\n    \n    # Cechy op√≥≈∫nie≈Ñ\n    'AIRLINE_DELAY_RATE': 'Procent op√≥≈∫nie≈Ñ danej linii',\n    'ORIGIN_DELAY_RATE': 'Procent op√≥≈∫nie≈Ñ lotniska wylotu',\n    \n    # Kategorie dystansu\n    'DISTANCE_BIN': 'Kategoria dystansu',\n    'DISTANCE_CATEGORY': 'Kategoria odleg≈Ço≈õci',\n    \n    # Cechy interakcyjne\n    'RUSH_AIRLINE': 'Ryzyko: godziny szczytu √ó op√≥≈∫nienia linii',\n    'HOLIDAY_ORIGIN': 'Ryzyko: ≈õwiƒôta √ó op√≥≈∫nienia lotniska',\n    'HOUR_AIRLINE': 'Ryzyko: godzina √ó op√≥≈∫nienia linii',\n    \n    # Data leakage (b≈Çƒôdna cecha)\n    'DELAY_LOG': 'üö® Logarytm op√≥≈∫nienia - wyciek danych'\n}\n\ndef get_feature_label(feature_name):\n    \"\"\"Zwraca opisowƒÖ etykietƒô dla cechy\"\"\"\n    return FEATURE_LABELS.get(feature_name, feature_name)\n\nprint(\"Mapowanie cech utworzone - bƒôdzie u≈ºywane w wykresach\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Label encoding\ncategorical_columns = ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'DISTANCE_BIN']\nfor col in categorical_columns:\n    le = LabelEncoder()\n    X_stage2[col] = le.fit_transform(X_stage2[col].astype(str))\n\n# Podzia≈Ç\nX_train2, X_test2, y_train2, y_test2 = train_test_split(\n    X_stage2, y_stage2, test_size=0.2, random_state=42, stratify=y_stage2\n)\n\n# SMOTE\nsmote = SMOTE(random_state=42, sampling_strategy=0.6)\nX_train2_smote, y_train2_smote = smote.fit_resample(X_train2, y_train2)\n\n# Trenowanie XGBoost\nprint(\"\\nTrenowanie modelu z data leakage...\")\nxgb_leakage = xgb.XGBClassifier(\n    n_estimators=150,\n    max_depth=8,\n    learning_rate=0.1,\n    subsample=0.8,\n    random_state=42,\n    n_jobs=-1\n)\n\nstart = time.time()\nxgb_leakage.fit(X_train2_smote, y_train2_smote)\nprint(f\"Czas trenowania: {time.time()-start:.1f}s\")\n\n# Predykcje z optymalizacjƒÖ threshold\ny_proba2 = xgb_leakage.predict_proba(X_test2)[:, 1]\n\n# Znajd≈∫ optymalny threshold\nthresholds = np.arange(0.3, 0.7, 0.02)\nf1_scores = []\nfor thresh in thresholds:\n    y_pred = (y_proba2 >= thresh).astype(int)\n    f1_scores.append(f1_score(y_test2, y_pred))\n\noptimal_threshold = thresholds[np.argmax(f1_scores)]\ny_pred2 = (y_proba2 >= optimal_threshold).astype(int)\n\n# Wyniki\nprint(\"\\n=== WYNIKI ETAP 2 (DATA LEAKAGE) ===\")\nprint(f\"Optymalny threshold: {optimal_threshold:.2f}\")\nprint(f\"Recall: {recall_score(y_test2, y_pred2)*100:.1f}% üöÄ\")\nprint(f\"Precision: {precision_score(y_test2, y_pred2)*100:.1f}%\")\nprint(f\"F1-Score: {f1_score(y_test2, y_pred2):.3f}\")\nprint(f\"ROC-AUC: {roc_auc_score(y_test2, y_proba2):.3f}\")\n\n# Confusion matrix\ncm2 = confusion_matrix(y_test2, y_pred2)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm2, annot=True, fmt='d', cmap='Reds')\nplt.title('Confusion Matrix - Etap 2 (Data Leakage)')\nplt.xlabel('Przewidywane')\nplt.ylabel('Rzeczywiste')\nplt.show()\n\n# Feature importance z opisowymi etykietami\nimportance2 = pd.DataFrame({\n    'feature': X_train2.columns,\n    'importance': xgb_leakage.feature_importances_\n}).sort_values('importance', ascending=False)\n\n# Dodaj opisowe etykiety\nimportance2['label'] = importance2['feature'].apply(get_feature_label)\n\nplt.figure(figsize=(12, 10))\ntop_features = importance2.head(15)\nplt.barh(range(len(top_features)), top_features['importance'])\n\n# Ustaw opisowe etykiety na osi Y\nplt.yticks(range(len(top_features)), top_features['label'])\n\nplt.xlabel('Wa≈ºno≈õƒá cechy', fontsize=12)\nplt.title('Top 15 najwa≈ºniejszych cech - Etap 2 (Data Leakage)', fontsize=14)\nplt.gca().invert_yaxis()\n\n# Podkre≈õl problematycznƒÖ cechƒô\nfor i, (feature, label) in enumerate(zip(top_features['feature'], top_features['label'])):\n    if feature == 'DELAY_LOG':\n        plt.gca().get_yticklabels()[i].set_color('red')\n        plt.gca().get_yticklabels()[i].set_weight('bold')\n        plt.gca().get_yticklabels()[i].set_fontsize(12)\n    else:\n        plt.gca().get_yticklabels()[i].set_fontsize(11)\n\n# Dodaj warto≈õci na s≈Çupkach\nfor i, v in enumerate(top_features['importance']):\n    plt.text(v + 0.002, i, f'{v:.3f}', va='center', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüö® UWAGA: DELAY_LOG jest najwa≈ºniejszƒÖ cechƒÖ - to dow√≥d data leakage!\")\nprint(\"Model 'oszukuje' u≈ºywajƒÖc informacji o op√≥≈∫nieniu do przewidywania op√≥≈∫nienia.\")\nprint(\"\\nOpisy najwa≈ºniejszych cech:\")\nfor _, row in top_features.head(5).iterrows():\n    print(f\"- {row['feature']}: {row['label']} (wa≈ºno≈õƒá: {row['importance']:.3f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETAP 3: Fast Optimized Model (62% recall)\n",
    "\n",
    "Model po usuniƒôciu data leakage, ale z b≈Çƒôdnym usuwaniem outlier√≥w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"ETAP 3: FAST OPTIMIZED MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Kopia danych dla etapu 3\n",
    "df_stage3 = df.copy()\n",
    "\n",
    "# üö® B≈ÅƒÑD: Usuwanie ekstremalnych op√≥≈∫nie≈Ñ!\n",
    "df_stage3 = df_stage3[(df_stage3['DEPARTURE_DELAY'] >= -30) & \n",
    "                      (df_stage3['DEPARTURE_DELAY'] <= 300)]  # Usuwamy trudne przypadki!\n",
    "\n",
    "print(f\"‚ö†Ô∏è UWAGA: Usuniƒôto {len(df) - len(df_stage3)} lot√≥w z ekstremalnymi op√≥≈∫nieniami\")\n",
    "\n",
    "# Sample\n",
    "if len(df_stage3) > 300000:\n",
    "    df_stage3 = df_stage3.sample(n=300000, random_state=42)\n",
    "\n",
    "print(f\"U≈ºywamy {len(df_stage3)} pr√≥bek\")\n",
    "\n",
    "# Zmienna docelowa\n",
    "df_stage3['DELAYED'] = (df_stage3['DEPARTURE_DELAY'] > 15).astype(int)\n",
    "\n",
    "# Feature engineering (21 cech, BEZ data leakage)\n",
    "df_stage3['DEPARTURE_HOUR'] = df_stage3['SCHEDULED_DEPARTURE'].astype(str).str.zfill(4).str[:2].astype(int)\n",
    "\n",
    "# Cyclical encoding\n",
    "df_stage3['HOUR_SIN'] = np.sin(2 * np.pi * df_stage3['DEPARTURE_HOUR'] / 24)\n",
    "df_stage3['HOUR_COS'] = np.cos(2 * np.pi * df_stage3['DEPARTURE_HOUR'] / 24)\n",
    "\n",
    "# Time features\n",
    "df_stage3['IS_RUSH_HOUR'] = (\n",
    "    ((df_stage3['DEPARTURE_HOUR'] >= 7) & (df_stage3['DEPARTURE_HOUR'] <= 9)) |\n",
    "    ((df_stage3['DEPARTURE_HOUR'] >= 17) & (df_stage3['DEPARTURE_HOUR'] <= 19))\n",
    ").astype(int)\n",
    "\n",
    "df_stage3['IS_WEEKEND'] = (df_stage3['DAY_OF_WEEK'].isin([6, 7])).astype(int)\n",
    "df_stage3['IS_FRIDAY'] = (df_stage3['DAY_OF_WEEK'] == 5).astype(int)\n",
    "\n",
    "# Airport congestion\n",
    "df_stage3['ORIGIN_CONGESTION'] = df_stage3.groupby('ORIGIN_AIRPORT')['ORIGIN_AIRPORT'].transform('count')\n",
    "df_stage3['DEST_CONGESTION'] = df_stage3.groupby('DESTINATION_AIRPORT')['DESTINATION_AIRPORT'].transform('count')\n",
    "\n",
    "# Airline delay rate\n",
    "airline_delay_rate3 = df_stage3.groupby('AIRLINE')['DELAYED'].mean()\n",
    "df_stage3['AIRLINE_DELAY_RATE'] = df_stage3['AIRLINE'].map(airline_delay_rate3)\n",
    "\n",
    "# Route popularity\n",
    "df_stage3['ROUTE'] = df_stage3['ORIGIN_AIRPORT'] + '_' + df_stage3['DESTINATION_AIRPORT']\n",
    "df_stage3['ROUTE_POPULARITY'] = df_stage3.groupby('ROUTE')['ROUTE'].transform('count')\n",
    "\n",
    "# Distance bins\n",
    "df_stage3['DISTANCE_BIN'] = pd.cut(df_stage3['DISTANCE'], \n",
    "                                   bins=[0, 500, 1000, 2000, 5000], \n",
    "                                   labels=['Short', 'Medium', 'Long', 'VeryLong'])\n",
    "\n",
    "# Cechy (21, bez data leakage)\n",
    "feature_columns_stage3 = [\n",
    "    'MONTH', 'DAY', 'DAY_OF_WEEK', 'DEPARTURE_HOUR',\n",
    "    'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT',\n",
    "    'DISTANCE', 'IS_WEEKEND', 'IS_FRIDAY', 'IS_RUSH_HOUR',\n",
    "    'HOUR_SIN', 'HOUR_COS',\n",
    "    'ORIGIN_CONGESTION', 'DEST_CONGESTION',\n",
    "    'AIRLINE_DELAY_RATE', 'ROUTE_POPULARITY',\n",
    "    'DISTANCE_BIN'\n",
    "]\n",
    "\n",
    "X_stage3 = df_stage3[feature_columns_stage3].copy()\n",
    "y_stage3 = df_stage3['DELAYED']\n",
    "\n",
    "print(f\"\\nCechy: {len(feature_columns_stage3)} (bez data leakage)\")\n",
    "print(f\"Procent op√≥≈∫nie≈Ñ: {y_stage3.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "categorical_columns = ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'DISTANCE_BIN']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_stage3[col] = le.fit_transform(X_stage3[col].astype(str))\n",
    "\n",
    "# Podzia≈Ç\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(\n",
    "    X_stage3, y_stage3, test_size=0.2, random_state=42, stratify=y_stage3\n",
    ")\n",
    "\n",
    "# Class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced', classes=np.unique(y_train3), y=y_train3\n",
    ")\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "# SMOTE\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.5)\n",
    "X_train3_smote, y_train3_smote = smote.fit_resample(X_train3, y_train3)\n",
    "\n",
    "# Trenowanie ensemble\n",
    "print(\"\\nTrenowanie modeli...\")\n",
    "\n",
    "# Random Forest\n",
    "rf3 = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=5,\n",
    "    class_weight=class_weight_dict,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf3.fit(X_train3_smote, y_train3_smote)\n",
    "\n",
    "# XGBoost\n",
    "scale_pos_weight = (y_train3 == 0).sum() / (y_train3 == 1).sum()\n",
    "xgb3 = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb3.fit(X_train3_smote, y_train3_smote)\n",
    "\n",
    "# LightGBM\n",
    "lgb3 = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    class_weight=class_weight_dict,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "lgb3.fit(X_train3_smote, y_train3_smote)\n",
    "\n",
    "# Ensemble\n",
    "ensemble3 = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf3),\n",
    "        ('xgb', xgb3),\n",
    "        ('lgb', lgb3)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "ensemble3.fit(X_train3, y_train3)\n",
    "\n",
    "# Optymalizacja threshold dla ensemble\n",
    "y_proba3 = ensemble3.predict_proba(X_test3)[:, 1]\n",
    "\n",
    "thresholds = np.arange(0.3, 0.7, 0.02)\n",
    "f1_scores = []\n",
    "for thresh in thresholds:\n",
    "    y_pred = (y_proba3 >= thresh).astype(int)\n",
    "    f1_scores.append(f1_score(y_test3, y_pred))\n",
    "\n",
    "optimal_threshold3 = thresholds[np.argmax(f1_scores)]\n",
    "y_pred3 = (y_proba3 >= optimal_threshold3).astype(int)\n",
    "\n",
    "# Wyniki\n",
    "print(\"\\n=== WYNIKI ETAP 3 (FAST OPTIMIZED) ===\")\n",
    "print(f\"Optymalny threshold: {optimal_threshold3:.2f}\")\n",
    "print(f\"Recall: {recall_score(y_test3, y_pred3)*100:.1f}% ‚úì\")\n",
    "print(f\"Precision: {precision_score(y_test3, y_pred3)*100:.1f}%\")\n",
    "print(f\"F1-Score: {f1_score(y_test3, y_pred3):.3f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test3, y_proba3):.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm3 = confusion_matrix(y_test3, y_pred3)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm3, annot=True, fmt='d', cmap='Oranges')\n",
    "plt.title('Confusion Matrix - Etap 3 (Fast Optimized)')\n",
    "plt.xlabel('Przewidywane')\n",
    "plt.ylabel('Rzeczywiste')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è PROBLEM: Wysoki recall, ale usunƒôli≈õmy najtrudniejsze przypadki (>300 min)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETAP 4: Final Optimized Model (54.4% recall)\n",
    "\n",
    "Uczciwy model zachowujƒÖcy WSZYSTKIE op√≥≈∫nienia, w≈ÇƒÖcznie z ekstremalnymi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"ETAP 4: FINAL OPTIMIZED MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Kopia danych dla etapu 4\n",
    "df_stage4 = df.copy()\n",
    "\n",
    "# ‚úì POPRAWKA: Zachowujemy WSZYSTKIE op√≥≈∫nienia!\n",
    "df_stage4 = df_stage4[df_stage4['DEPARTURE_DELAY'] >= -60]  # Tylko ekstremalne b≈Çƒôdy danych\n",
    "\n",
    "print(f\"‚úì Zachowano wszystkie op√≥≈∫nienia, w≈ÇƒÖcznie z ekstremalnymi\")\n",
    "print(f\"Max op√≥≈∫nienie: {df_stage4['DEPARTURE_DELAY'].max():.0f} minut\")\n",
    "print(f\"Op√≥≈∫nienia >300 min: {(df_stage4['DEPARTURE_DELAY'] > 300).sum()}\")\n",
    "\n",
    "# Sample\n",
    "if len(df_stage4) > 300000:\n",
    "    df_stage4 = df_stage4.sample(n=300000, random_state=42)\n",
    "\n",
    "# Zmienna docelowa\n",
    "df_stage4['DELAYED'] = (df_stage4['DEPARTURE_DELAY'] > 15).astype(int)\n",
    "\n",
    "# Zaawansowany feature engineering (28 cech)\n",
    "df_stage4['DEPARTURE_HOUR'] = df_stage4['SCHEDULED_DEPARTURE'].astype(str).str.zfill(4).str[:2].astype(int)\n",
    "df_stage4['DEPARTURE_MINUTE'] = df_stage4['SCHEDULED_DEPARTURE'].astype(str).str.zfill(4).str[2:].astype(int)\n",
    "\n",
    "# Cyclical encoding\n",
    "df_stage4['HOUR_SIN'] = np.sin(2 * np.pi * df_stage4['DEPARTURE_HOUR'] / 24)\n",
    "df_stage4['HOUR_COS'] = np.cos(2 * np.pi * df_stage4['DEPARTURE_HOUR'] / 24)\n",
    "df_stage4['MONTH_SIN'] = np.sin(2 * np.pi * df_stage4['MONTH'] / 12)\n",
    "df_stage4['MONTH_COS'] = np.cos(2 * np.pi * df_stage4['MONTH'] / 12)\n",
    "\n",
    "# Time-based features\n",
    "df_stage4['IS_RUSH_HOUR'] = (\n",
    "    ((df_stage4['DEPARTURE_HOUR'] >= 7) & (df_stage4['DEPARTURE_HOUR'] <= 9)) |\n",
    "    ((df_stage4['DEPARTURE_HOUR'] >= 17) & (df_stage4['DEPARTURE_HOUR'] <= 19))\n",
    ").astype(int)\n",
    "\n",
    "df_stage4['IS_LATE_NIGHT'] = (\n",
    "    (df_stage4['DEPARTURE_HOUR'] >= 22) | (df_stage4['DEPARTURE_HOUR'] <= 5)\n",
    ").astype(int)\n",
    "\n",
    "df_stage4['IS_EARLY_MORNING'] = (\n",
    "    (df_stage4['DEPARTURE_HOUR'] >= 4) & (df_stage4['DEPARTURE_HOUR'] <= 6)\n",
    ").astype(int)\n",
    "\n",
    "# Weekend/Holiday\n",
    "df_stage4['IS_WEEKEND'] = (df_stage4['DAY_OF_WEEK'].isin([6, 7])).astype(int)\n",
    "df_stage4['IS_FRIDAY'] = (df_stage4['DAY_OF_WEEK'] == 5).astype(int)\n",
    "df_stage4['IS_MONDAY'] = (df_stage4['DAY_OF_WEEK'] == 1).astype(int)\n",
    "\n",
    "df_stage4['IS_HOLIDAY_SEASON'] = (\n",
    "    ((df_stage4['MONTH'] == 12) & (df_stage4['DAY'] >= 20)) |\n",
    "    ((df_stage4['MONTH'] == 11) & (df_stage4['DAY'] >= 22) & (df_stage4['DAY'] <= 28)) |\n",
    "    ((df_stage4['MONTH'] == 7) & (df_stage4['DAY'] <= 7)) |\n",
    "    ((df_stage4['MONTH'] == 1) & (df_stage4['DAY'] <= 3))\n",
    ").astype(int)\n",
    "\n",
    "# Airport features\n",
    "origin_counts = df_stage4['ORIGIN_AIRPORT'].value_counts()\n",
    "dest_counts = df_stage4['DESTINATION_AIRPORT'].value_counts()\n",
    "df_stage4['ORIGIN_BUSY'] = df_stage4['ORIGIN_AIRPORT'].map(origin_counts)\n",
    "df_stage4['DEST_BUSY'] = df_stage4['DESTINATION_AIRPORT'].map(dest_counts)\n",
    "\n",
    "# Route features\n",
    "df_stage4['ROUTE'] = df_stage4['ORIGIN_AIRPORT'] + '_' + df_stage4['DESTINATION_AIRPORT']\n",
    "df_stage4['ROUTE_FREQ'] = df_stage4['ROUTE'].map(df_stage4['ROUTE'].value_counts())\n",
    "\n",
    "# Airline features\n",
    "airline_delay_rate = df_stage4.groupby('AIRLINE')['DELAYED'].mean()\n",
    "df_stage4['AIRLINE_DELAY_RATE'] = df_stage4['AIRLINE'].map(airline_delay_rate)\n",
    "\n",
    "# Origin airport delay rate\n",
    "origin_delay_rate = df_stage4.groupby('ORIGIN_AIRPORT')['DELAYED'].mean()\n",
    "df_stage4['ORIGIN_DELAY_RATE'] = df_stage4['ORIGIN_AIRPORT'].map(origin_delay_rate)\n",
    "\n",
    "# Distance features\n",
    "df_stage4['DISTANCE_BIN'] = pd.cut(df_stage4['DISTANCE'], \n",
    "                                   bins=[0, 500, 1000, 2000, 5000], \n",
    "                                   labels=['Short', 'Medium', 'Long', 'VeryLong'])\n",
    "\n",
    "# Interaction features\n",
    "df_stage4['RUSH_AIRLINE'] = df_stage4['IS_RUSH_HOUR'] * df_stage4['AIRLINE_DELAY_RATE']\n",
    "df_stage4['HOLIDAY_ORIGIN'] = df_stage4['IS_HOLIDAY_SEASON'] * df_stage4['ORIGIN_DELAY_RATE']\n",
    "df_stage4['HOUR_AIRLINE'] = df_stage4['DEPARTURE_HOUR'] * df_stage4['AIRLINE_DELAY_RATE'] / 24\n",
    "\n",
    "# Cechy finalne (28)\n",
    "feature_columns_stage4 = [\n",
    "    # Base features\n",
    "    'MONTH', 'DAY', 'DAY_OF_WEEK', 'DEPARTURE_HOUR',\n",
    "    'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'DISTANCE',\n",
    "    \n",
    "    # Time features\n",
    "    'IS_WEEKEND', 'IS_FRIDAY', 'IS_MONDAY', 'IS_RUSH_HOUR', \n",
    "    'IS_LATE_NIGHT', 'IS_EARLY_MORNING',\n",
    "    'HOUR_SIN', 'HOUR_COS', 'MONTH_SIN', 'MONTH_COS',\n",
    "    \n",
    "    # Holiday\n",
    "    'IS_HOLIDAY_SEASON',\n",
    "    \n",
    "    # Airport/Route features\n",
    "    'ORIGIN_BUSY', 'DEST_BUSY', 'ROUTE_FREQ',\n",
    "    'AIRLINE_DELAY_RATE', 'ORIGIN_DELAY_RATE',\n",
    "    \n",
    "    # Distance\n",
    "    'DISTANCE_BIN',\n",
    "    \n",
    "    # Interactions\n",
    "    'RUSH_AIRLINE', 'HOLIDAY_ORIGIN', 'HOUR_AIRLINE'\n",
    "]\n",
    "\n",
    "X_stage4 = df_stage4[feature_columns_stage4].copy()\n",
    "y_stage4 = df_stage4['DELAYED']\n",
    "\n",
    "print(f\"\\nCechy: {len(feature_columns_stage4)}\")\n",
    "print(f\"Procent op√≥≈∫nie≈Ñ: {y_stage4.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Label encoding\ncategorical_columns = ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'DISTANCE_BIN']\nfor col in categorical_columns:\n    le = LabelEncoder()\n    X_stage4[col] = le.fit_transform(X_stage4[col].astype(str))\n\n# Podzia≈Ç\nX_train4, X_test4, y_train4, y_test4 = train_test_split(\n    X_stage4, y_stage4, test_size=0.2, random_state=42, stratify=y_stage4\n)\n\n# Class weights\nclass_weights = class_weight.compute_class_weight(\n    'balanced', classes=np.unique(y_train4), y=y_train4\n)\nclass_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n\n# SMOTE\nsmote = SMOTE(random_state=42, sampling_strategy=0.6)\nX_train4_smote, y_train4_smote = smote.fit_resample(X_train4, y_train4)\n\n# Trenowanie najlepszego modelu - XGBoost\nprint(\"\\nTrenowanie finalnego modelu XGBoost...\")\nscale_pos_weight = (y_train4 == 0).sum() / (y_train4 == 1).sum()\n\nxgb_final = xgb.XGBClassifier(\n    n_estimators=150,\n    max_depth=8,\n    learning_rate=0.1,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    scale_pos_weight=scale_pos_weight,\n    gamma=0.1,\n    random_state=42,\n    n_jobs=-1\n)\n\nstart = time.time()\nxgb_final.fit(X_train4_smote, y_train4_smote)\nprint(f\"Czas trenowania: {time.time()-start:.1f}s\")\n\n# Optymalizacja threshold\ny_proba4 = xgb_final.predict_proba(X_test4)[:, 1]\n\nthresholds = np.arange(0.3, 0.7, 0.02)\nf1_scores = []\nfor thresh in thresholds:\n    y_pred = (y_proba4 >= thresh).astype(int)\n    f1_scores.append(f1_score(y_test4, y_pred))\n\noptimal_threshold4 = thresholds[np.argmax(f1_scores)]\ny_pred4 = (y_proba4 >= optimal_threshold4).astype(int)\n\n# Wyniki\nprint(\"\\n=== WYNIKI ETAP 4 (FINAL MODEL) ===\")\nprint(f\"Optymalny threshold: {optimal_threshold4:.2f}\")\nprint(f\"Recall: {recall_score(y_test4, y_pred4)*100:.1f}%\")\nprint(f\"Precision: {precision_score(y_test4, y_pred4)*100:.1f}%\")\nprint(f\"F1-Score: {f1_score(y_test4, y_pred4):.3f}\")\nprint(f\"ROC-AUC: {roc_auc_score(y_test4, y_proba4):.3f}\")\n\n# Analiza ekstremalnych op√≥≈∫nie≈Ñ\ntest_indices = X_test4.index\nextreme_delays_mask = df_stage4.loc[test_indices, 'DEPARTURE_DELAY'] > 300\nif extreme_delays_mask.sum() > 0:\n    extreme_y_true = y_test4[extreme_delays_mask]\n    extreme_y_pred = y_pred4[extreme_delays_mask]\n    extreme_recall = recall_score(extreme_y_true, extreme_y_pred)\n    print(f\"\\nRecall dla ekstremalnych op√≥≈∫nie≈Ñ (>300 min): {extreme_recall*100:.1f}%\")\n    print(f\"Wykryto {extreme_y_pred.sum()}/{len(extreme_y_true)} ekstremalnych op√≥≈∫nie≈Ñ\")\n\n# Confusion matrix\ncm4 = confusion_matrix(y_test4, y_pred4)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm4, annot=True, fmt='d', cmap='Greens')\nplt.title('Confusion Matrix - Etap 4 (Final Model)')\nplt.xlabel('Przewidywane')\nplt.ylabel('Rzeczywiste')\nplt.show()\n\n# Feature importance z opisowymi etykietami\nimportance4 = pd.DataFrame({\n    'feature': X_train4.columns,\n    'importance': xgb_final.feature_importances_\n}).sort_values('importance', ascending=False)\n\n# Dodaj opisowe etykiety\nimportance4['label'] = importance4['feature'].apply(get_feature_label)\n\nplt.figure(figsize=(12, 10))\ntop_features = importance4.head(15)\nplt.barh(range(len(top_features)), top_features['importance'])\n\n# Ustaw opisowe etykiety na osi Y\nplt.yticks(range(len(top_features)), top_features['label'])\n\nplt.xlabel('Wa≈ºno≈õƒá cechy', fontsize=12)\nplt.title('Top 15 najwa≈ºniejszych cech - Final Model (Uczciwy model)', fontsize=14)\nplt.gca().invert_yaxis()\n\n# Ulepszone kolorowanie wed≈Çug typu cechy\ncolors = []\nfor feature in top_features['feature']:\n    # Cechy czasowe bezpo≈õrednie\n    if feature in ['IS_RUSH_HOUR', 'IS_WEEKEND', 'IS_FRIDAY', 'IS_MONDAY', \n                   'IS_LATE_NIGHT', 'IS_EARLY_MORNING', 'IS_HOLIDAY_SEASON']:\n        colors.append('coral')  # Cechy czasowe binarne\n    # Cechy czasowe cykliczne\n    elif feature in ['HOUR_SIN', 'HOUR_COS', 'MONTH_SIN', 'MONTH_COS']:\n        colors.append('lightsalmon')  # Cechy czasowe cykliczne\n    # Podstawowe cechy czasowe\n    elif feature in ['MONTH', 'DAY', 'DAY_OF_WEEK', 'DEPARTURE_HOUR']:\n        colors.append('peachpuff')  # Podstawowe cechy czasowe\n    # Cechy lotniskowe\n    elif 'ORIGIN' in feature or 'DEST' in feature or 'AIRPORT' in feature:\n        colors.append('skyblue')  # Cechy lotniskowe\n    # Cechy linii lotniczych\n    elif 'AIRLINE' in feature:\n        colors.append('lightgreen')  # Cechy linii lotniczych\n    # Cechy dystansu\n    elif 'DISTANCE' in feature:\n        colors.append('gold')  # Cechy dystansu\n    # Cechy tras\n    elif 'ROUTE' in feature:\n        colors.append('plum')  # Cechy tras\n    # Cechy interakcyjne/ryzyko\n    elif feature in ['RUSH_AIRLINE', 'HOLIDAY_ORIGIN', 'HOUR_AIRLINE']:\n        colors.append('lightcoral')  # Cechy interakcyjne\n    else:\n        colors.append('lightgray')  # Pozosta≈Çe\n\nbars = plt.barh(range(len(top_features)), top_features['importance'], color=colors)\n\n# Dodaj warto≈õci na s≈Çupkach\nfor i, v in enumerate(top_features['importance']):\n    plt.text(v + 0.002, i, f'{v:.3f}', va='center', fontsize=10)\n\n# Ulepszona legenda\nfrom matplotlib.patches import Patch\nlegend_elements = [\n    Patch(facecolor='coral', label='Cechy czasowe (binarne)'),\n    Patch(facecolor='lightsalmon', label='Cechy czasowe (cykliczne)'),\n    Patch(facecolor='peachpuff', label='Cechy czasowe (podstawowe)'),\n    Patch(facecolor='skyblue', label='Cechy lotniskowe'),\n    Patch(facecolor='lightgreen', label='Cechy linii lotniczych'),\n    Patch(facecolor='gold', label='Cechy dystansu'),\n    Patch(facecolor='plum', label='Cechy tras'),\n    Patch(facecolor='lightcoral', label='Cechy interakcyjne/ryzyko')\n]\nplt.legend(handles=legend_elements, loc='lower right', fontsize=9, ncol=2)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚úì Model uczciwie radzi sobie ze WSZYSTKIMI op√≥≈∫nieniami\")\nprint(\"‚úì Najwa≈ºniejsze cechy sƒÖ zwiƒÖzane z czasem (godziny szczytu) i lotniskami\")\nprint(\"\\nOpisy TOP 5 najwa≈ºniejszych cech:\")\nfor i, row in top_features.head(5).iterrows():\n    print(f\"{i+1}. {row['feature']}: {row['label']} (wa≈ºno≈õƒá: {row['importance']:.3f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podsumowanie: Por√≥wnanie wszystkich etap√≥w"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Zbierz wyniki ze wszystkich etap√≥w\nresults_summary = pd.DataFrame({\n    'Etap': ['1: Baseline', '2: Data Leakage', '3: Fast Optimized', '4: Final Model'],\n    'Recall': [\n        recall_score(y_test1, y_pred_xgb1)*100,  # Etap 1\n        recall_score(y_test2, y_pred2)*100,      # Etap 2\n        recall_score(y_test3, y_pred3)*100,      # Etap 3\n        recall_score(y_test4, y_pred4)*100       # Etap 4\n    ],\n    'F1-Score': [\n        f1_score(y_test1, y_pred_xgb1),\n        f1_score(y_test2, y_pred2),\n        f1_score(y_test3, y_pred3),\n        f1_score(y_test4, y_pred4)\n    ],\n    'Cechy': [12, 27, 21, 28],\n    'Problem': [\n        'Zbyt prosty model',\n        'Data leakage (DELAY_LOG)',\n        'Usuniƒôto outliery >300 min',\n        'Uczciwy model ze wszystkim'\n    ]\n})\n\nprint(\"=== PODSUMOWANIE WSZYSTKICH ETAP√ìW ===\")\nprint(results_summary.to_string(index=False))\n\n# Wizualizacja ewolucji\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Recall\nbars1 = ax1.bar(results_summary['Etap'], results_summary['Recall'], \n                color=['blue', 'red', 'orange', 'green'])\nax1.set_ylabel('Recall (%)')\nax1.set_title('Ewolucja Recall przez etapy')\nax1.set_ylim(0, 110)\n\n# Dodaj warto≈õci na s≈Çupkach\nfor i, bar in enumerate(bars1):\n    height = bar.get_height()\n    # Dla wysokich s≈Çupk√≥w (>90%) umie≈õƒá etykietƒô wewnƒÖtrz s≈Çupka\n    if height > 90:\n        ax1.text(bar.get_x() + bar.get_width()/2., height - 5,\n                 f'{height:.1f}%', ha='center', va='top', \n                 color='white', fontweight='bold')\n    else:\n        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n                 f'{height:.1f}%', ha='center', va='bottom')\n\n# F1-Score - ZWIƒòKSZONY LIMIT DO 1.0\nbars2 = ax2.bar(results_summary['Etap'], results_summary['F1-Score'], \n                color=['blue', 'red', 'orange', 'green'])\nax2.set_ylabel('F1-Score')\nax2.set_title('Ewolucja F1-Score przez etapy')\nax2.set_ylim(0, 1.0)  # Zwiƒôkszony do 1.0\n\n# Dodaj warto≈õci na s≈Çupkach - WSZYSTKIE WEWNƒÑTRZ DLA SP√ìJNO≈öCI\nfor i, bar in enumerate(bars2):\n    height = bar.get_height()\n    # Umie≈õƒá wszystkie etykiety wewnƒÖtrz s≈Çupk√≥w dla sp√≥jno≈õci z lewym wykresem\n    ax2.text(bar.get_x() + bar.get_width()/2., height - 0.03,\n             f'{height:.3f}', ha='center', va='top',\n             color='white', fontweight='bold', fontsize=11)\n\n# Dodaj adnotacje\nax1.annotate('Podejrzane!', \n            xy=(1, results_summary.loc[1, 'Recall']), \n            xytext=(1, 85),\n            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n            ha='center', fontsize=10, color='red', fontweight='bold')\n\nax2.annotate('Sztucznie wysoki\\n(data leakage)', \n            xy=(1, results_summary.loc[1, 'F1-Score']), \n            xytext=(1, 0.85),\n            arrowprops=dict(arrowstyle='->', color='red', lw=1.5),\n            ha='center', fontsize=9, color='red')\n\nplt.tight_layout()\nplt.show()\n\n# Krzywe ROC\nplt.figure(figsize=(10, 8))\n\n# Oblicz krzywe ROC dla ka≈ºdego etapu\nfpr1, tpr1, _ = roc_curve(y_test1, xgb_baseline.predict_proba(X_test1)[:, 1])\nfpr2, tpr2, _ = roc_curve(y_test2, y_proba2)\nfpr3, tpr3, _ = roc_curve(y_test3, y_proba3)\nfpr4, tpr4, _ = roc_curve(y_test4, y_proba4)\n\n# Wykresy\nplt.plot(fpr1, tpr1, label=f'Etap 1: Baseline (AUC = {roc_auc_score(y_test1, xgb_baseline.predict_proba(X_test1)[:, 1]):.3f})', linewidth=2)\nplt.plot(fpr2, tpr2, label=f'Etap 2: Data Leakage (AUC = {roc_auc_score(y_test2, y_proba2):.3f})', linewidth=2, linestyle='--')\nplt.plot(fpr3, tpr3, label=f'Etap 3: Fast Optimized (AUC = {roc_auc_score(y_test3, y_proba3):.3f})', linewidth=2)\nplt.plot(fpr4, tpr4, label=f'Etap 4: Final Model (AUC = {roc_auc_score(y_test4, y_proba4):.3f})', linewidth=3)\n\nplt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Losowy klasyfikator')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Krzywe ROC - Por√≥wnanie wszystkich etap√≥w')\nplt.legend(loc='lower right')\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"\\n=== KLUCZOWE WNIOSKI ===\")\nprint(\"1. Etap 1 (Baseline): Zbyt konserwatywny model - tylko 10% recall\")\nprint(\"2. Etap 2 (Data Leakage): Fa≈Çszywie wysoki recall 77.5% przez u≈ºycie DELAY_LOG\")\nprint(\"3. Etap 3 (Fast Optimized): Dobry recall 62%, ale osiƒÖgniƒôty przez usuniƒôcie trudnych przypadk√≥w\")\nprint(\"4. Etap 4 (Final Model): Uczciwy recall 54.4% na WSZYSTKICH danych\")\nprint(\"\\n‚úì Najlepszy uczciwy model: XGBoost z 28 cechami, F1=0.491, ROC-AUC=0.769\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza b≈Çƒôd√≥w i dalsze kroki\n",
    "\n",
    "Zobaczmy, gdzie model finalny ma najwiƒôksze problemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analiza b≈Çƒôd√≥w\n",
    "test_df = df_stage4.loc[X_test4.index].copy()\n",
    "test_df['y_true'] = y_test4\n",
    "test_df['y_pred'] = y_pred4\n",
    "test_df['y_proba'] = y_proba4\n",
    "\n",
    "# False Negatives (missed delays)\n",
    "false_negatives = test_df[(test_df['y_true'] == 1) & (test_df['y_pred'] == 0)]\n",
    "print(f\"False Negatives (niewykryte op√≥≈∫nienia): {len(false_negatives)}\")\n",
    "\n",
    "# Analiza wed≈Çug wielko≈õci op√≥≈∫nienia\n",
    "delay_bins = [15, 30, 60, 120, 300, 2000]\n",
    "delay_labels = ['15-30 min', '30-60 min', '60-120 min', '120-300 min', '>300 min']\n",
    "\n",
    "test_df['DELAY_BIN'] = pd.cut(test_df['DEPARTURE_DELAY'], bins=delay_bins, labels=delay_labels, include_lowest=False)\n",
    "\n",
    "# Recall dla ka≈ºdej kategorii op√≥≈∫nienia\n",
    "recall_by_delay = test_df[test_df['y_true'] == 1].groupby('DELAY_BIN').apply(\n",
    "    lambda x: (x['y_pred'] == 1).sum() / len(x) * 100\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "recall_by_delay.plot(kind='bar', color='coral')\n",
    "plt.title('Recall wed≈Çug wielko≈õci op√≥≈∫nienia')\n",
    "plt.xlabel('Kategoria op√≥≈∫nienia')\n",
    "plt.ylabel('Recall (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.axhline(y=50, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Dodaj warto≈õci na s≈Çupkach\n",
    "for i, v in enumerate(recall_by_delay):\n",
    "    plt.text(i, v + 1, f'{v:.1f}%', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRecall wed≈Çug wielko≈õci op√≥≈∫nienia:\")\n",
    "for delay_cat, recall in recall_by_delay.items():\n",
    "    print(f\"{delay_cat}: {recall:.1f}%\")\n",
    "\n",
    "# Najczƒôstsze b≈Çƒôdy wed≈Çug lotnisk\n",
    "print(\"\\n=== LOTNISKA Z NAJNI≈ªSZYM RECALL ===\")\n",
    "airport_performance = test_df[test_df['y_true'] == 1].groupby('ORIGIN_AIRPORT').agg({\n",
    "    'y_pred': ['sum', 'count']\n",
    "})\n",
    "airport_performance.columns = ['detected', 'total']\n",
    "airport_performance['recall'] = airport_performance['detected'] / airport_performance['total'] * 100\n",
    "airport_performance = airport_performance[airport_performance['total'] >= 10]  # Min 10 op√≥≈∫nie≈Ñ\n",
    "\n",
    "worst_airports = airport_performance.nsmallest(10, 'recall')\n",
    "print(worst_airports[['total', 'detected', 'recall']].round(1))\n",
    "\n",
    "print(\"\\n=== PROPOZYCJE DALSZYCH ULEPSZE≈É ===\")\n",
    "print(\"1. Model dwuetapowy:\")\n",
    "print(\"   - Etap 1: Klasyfikacja normal/extreme delay\")\n",
    "print(\"   - Etap 2: Dedykowane modele dla ka≈ºdej grupy\")\n",
    "print(\"\\n2. Dodatkowe cechy:\")\n",
    "print(\"   - Dane pogodowe (mo≈ºna symulowaƒá na podstawie sezonu/lokalizacji)\")\n",
    "print(\"   - Agregacje historyczne (≈õrednie op√≥≈∫nienie na trasie ostatnie 7 dni)\")\n",
    "print(\"   - Cechy ekonomiczne (ceny paliwa, wska≈∫niki)\")\n",
    "print(\"\\n3. Techniki modelowania:\")\n",
    "print(\"   - Stacking ensemble z meta-learnerem\")\n",
    "print(\"   - Custom loss function z wiƒôkszƒÖ wagƒÖ dla du≈ºych op√≥≈∫nie≈Ñ\")\n",
    "print(\"   - Neural network jako dodatkowy model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zapisanie najlepszego modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapisz model i wa≈ºne informacje\n",
    "import joblib\n",
    "\n",
    "# Zapisz model\n",
    "joblib.dump(xgb_final, 'best_flight_delay_model.pkl')\n",
    "\n",
    "# Zapisz metadane\n",
    "model_metadata = {\n",
    "    'model_type': 'XGBoost',\n",
    "    'features': feature_columns_stage4,\n",
    "    'n_features': len(feature_columns_stage4),\n",
    "    'optimal_threshold': float(optimal_threshold4),\n",
    "    'performance': {\n",
    "        'recall': float(recall_score(y_test4, y_pred4)),\n",
    "        'precision': float(precision_score(y_test4, y_pred4)),\n",
    "        'f1_score': float(f1_score(y_test4, y_pred4)),\n",
    "        'roc_auc': float(roc_auc_score(y_test4, y_proba4))\n",
    "    },\n",
    "    'training_samples': len(X_train4),\n",
    "    'test_samples': len(X_test4)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('model_metadata.json', 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(\"‚úì Model zapisany jako 'best_flight_delay_model.pkl'\")\n",
    "print(\"‚úì Metadane zapisane jako 'model_metadata.json'\")\n",
    "print(\"\\n=== PROJEKT ZAKO≈ÉCZONY ===\")\n",
    "print(f\"Najlepszy model: {model_metadata['model_type']}\")\n",
    "print(f\"F1-Score: {model_metadata['performance']['f1_score']:.3f}\")\n",
    "print(f\"ROC-AUC: {model_metadata['performance']['roc_auc']:.3f}\")\n",
    "print(f\"Recall: {model_metadata['performance']['recall']*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}